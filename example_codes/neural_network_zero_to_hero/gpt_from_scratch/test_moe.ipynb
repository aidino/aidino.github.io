{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fc823d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding how gating works\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embed=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fake multihead attention output\n",
    "mh_output = torch.randn(2, 4, n_embed) # (B, T, C) = (2, 4, 32) = (batch_size, block_size, n_embed)\n",
    "mh_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topkgate_linear = nn.Linear(n_embed, num_experts)\n",
    "logits = topkgate_linear(mh_output)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1266,  0.3873, -0.4022, -0.5019],\n",
       "         [-1.4505,  0.4356, -0.5226, -0.9259],\n",
       "         [-0.0429, -0.1706, -0.1338, -0.3950],\n",
       "         [-0.2503,  0.3080, -0.1050, -0.5048]],\n",
       "\n",
       "        [[-0.3012,  0.7607, -1.3323,  0.7659],\n",
       "         [-0.9557,  0.1939, -0.0320, -1.3697],\n",
       "         [-0.2010, -0.5835,  0.3432, -0.2241],\n",
       "         [-1.1562,  0.3388, -0.6097,  0.2768]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy top_k giá trị lớn nhất và chỉ số của chúng từ logits theo chiều cuối cùng (dim=-1).\n",
    "top_k_logit, top_k_idx = logits.topk(top_k, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3873,  0.1266],\n",
       "         [ 0.4356, -0.5226],\n",
       "         [-0.0429, -0.1338],\n",
       "         [ 0.3080, -0.1050]],\n",
       "\n",
       "        [[ 0.7659,  0.7607],\n",
       "         [ 0.1939, -0.0320],\n",
       "         [ 0.3432, -0.2010],\n",
       "         [ 0.3388,  0.2768]]], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0],\n",
       "         [1, 2],\n",
       "         [0, 2],\n",
       "         [1, 2]],\n",
       "\n",
       "        [[3, 1],\n",
       "         [1, 2],\n",
       "         [2, 0],\n",
       "         [1, 3]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy đầu ra sparse gating bằng cách chỉ giữ lại k giá trị hàng đầu tại chỉ số tương ứng của chúng dọc theo chiều cuối cùng. Điền phần còn lại bằng ‘-inf’ và truyền qua hàm kích hoạt softmax. Điều này đẩy các giá trị ‘-inf’ về không, làm cho hai giá trị hàng đầu được nhấn mạnh hơn và tổng bằng 1. Tổng bằng 1 này giúp ích cho việc tính trọng số của đầu ra expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo một tensor zeros có cùng shape với logits (tức (2, 4, num_experts)).\n",
    "# Các phần tử được điền giá trị âm vô cực (-inf).\n",
    "# Mục đích là để tạo một mask mà chỉ giữ lại các giá trị top-k.\n",
    "zeros = torch.full_like(logits, float('-inf'))\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1266,  0.3873,    -inf,    -inf],\n",
       "         [   -inf,  0.4356, -0.5226,    -inf],\n",
       "         [-0.0429,    -inf, -0.1338,    -inf],\n",
       "         [   -inf,  0.3080, -0.1050,    -inf]],\n",
       "\n",
       "        [[   -inf,  0.7607,    -inf,  0.7659],\n",
       "         [   -inf,  0.1939, -0.0320,    -inf],\n",
       "         [-0.2010,    -inf,  0.3432,    -inf],\n",
       "         [   -inf,  0.3388,    -inf,  0.2768]]], grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo tensor thưa bằng cách đặt các giá trị top_k_logit vào vị trí được chỉ định bởi top_k_idx.\n",
    "# scatter thay thế các giá trị -inf tại các chỉ số top_k_idx bằng top_k_logit.\n",
    "# Kết quả sparse_logits có shape (2, 4, num_experts), trong đó:\n",
    "# Chỉ có top_k giá trị khác -inf ở mỗi chuỗi\n",
    "# Các vị trí khác vẫn là -inf\n",
    "sparse_logits = zeros.scatter(-1, top_k_idx, top_k_logit)\n",
    "sparse_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4352, 0.5648, 0.0000, 0.0000],\n",
       "         [0.0000, 0.7228, 0.2772, 0.0000],\n",
       "         [0.5227, 0.0000, 0.4773, 0.0000],\n",
       "         [0.0000, 0.6018, 0.3982, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.4987, 0.0000, 0.5013],\n",
       "         [0.0000, 0.5562, 0.4438, 0.0000],\n",
       "         [0.3672, 0.0000, 0.6328, 0.0000],\n",
       "         [0.0000, 0.5155, 0.0000, 0.4845]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_output= F.softmax(sparse_logits, dim=-1)\n",
    "gating_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chỉ dùng top-k sẽ rất có khả năng có những expert được chọn nhiều lần, điều đó dẫn đến không cân bằng tải.\n",
    "\n",
    "=> Phương pháp: Noisy top-k Gating for load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the top k router module \n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear =nn.Linear(n_embed, num_experts)\n",
    "    \n",
    "    def forward(self, mh_ouput):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1) \n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        #scatter thay thế các giá trị -inf tại các chỉ số top_k_idx bằng top_k_logit.\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 4]),\n",
       " tensor([[[0.0000, 0.0000, 0.5394, 0.4606],\n",
       "          [0.0000, 0.8703, 0.0000, 0.1297],\n",
       "          [0.0000, 0.6878, 0.3122, 0.0000],\n",
       "          [0.6251, 0.0000, 0.3749, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.7266, 0.0000, 0.2734],\n",
       "          [0.4494, 0.0000, 0.5506, 0.0000],\n",
       "          [0.2100, 0.0000, 0.7900, 0.0000],\n",
       "          [0.7678, 0.2322, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[2, 3],\n",
       "          [1, 3],\n",
       "          [1, 2],\n",
       "          [0, 2]],\n",
       " \n",
       "         [[1, 3],\n",
       "          [2, 0],\n",
       "          [2, 0],\n",
       "          [0, 1]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing this out:\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 32\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the above to accomodate noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    \n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 8]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.4747, 0.0000, 0.5253, 0.0000],\n",
       "          [0.0000, 0.4845, 0.5155, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.9715, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 0.0000],\n",
       "          [0.4486, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5514]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.4555, 0.0000, 0.0000, 0.5445, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5230, 0.0000, 0.4770, 0.0000],\n",
       "          [0.2455, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7545],\n",
       "          [0.0000, 0.0000, 0.5009, 0.0000, 0.0000, 0.0000, 0.4991, 0.0000]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[6, 4],\n",
       "          [2, 1],\n",
       "          [1, 6],\n",
       "          [7, 0]],\n",
       " \n",
       "         [[6, 3],\n",
       "          [4, 6],\n",
       "          [7, 0],\n",
       "          [2, 6]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing this out, again:\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices\n",
    "#It works!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sparse Mixture of Experts module\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Expert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     10\u001b[0m mh_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, n_embd)  \u001b[38;5;66;03m# Example multi-head attention output\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sparse_moe \u001b[38;5;241m=\u001b[39m \u001b[43mSparseMoE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m final_output \u001b[38;5;241m=\u001b[39m sparse_moe(mh_output)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of the final output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_output\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mSparseMoE.__init__\u001b[0;34m(self, n_embed, num_experts, top_k)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(SparseMoE, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrouter \u001b[38;5;241m=\u001b[39m NoisyTopkRouter(n_embed, num_experts, top_k)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([Expert(n_embed) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_experts)])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k \u001b[38;5;241m=\u001b[39m top_k\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(SparseMoE, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrouter \u001b[38;5;241m=\u001b[39m NoisyTopkRouter(n_embed, num_experts, top_k)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mExpert\u001b[49m(n_embed) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_experts)])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k \u001b[38;5;241m=\u001b[39m top_k\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Expert' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Let's test this out\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "dropout=0.1\n",
    "\n",
    "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
