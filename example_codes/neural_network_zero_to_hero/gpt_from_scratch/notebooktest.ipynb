{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"than_dieu_dai_hiep.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in charactets:  3815622\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in charactets: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kim Dung\n",
      "Thần Điêu Đại Hiệp\n",
      "Hồi 1\n",
      "Người dị khách trên bờ hồ\n",
      "Giang Nam, một dải đất hữu tình gồm nhiều thắng cảnh mà xưa nay khách giang hồ đã từng lưu gót, không một ai không quyến luyến. Bốn mùa đều có hoa nở: Xuân có thuỳ liễu xinh tươi, hạ có anh đào trắng xoá. Nhất là vào đầu thu, trong hồ sen không ngớt những tiếng hát véo von cũ những thân nhân quý tộc. Họ tiêu khiển bằng thú chèo ghe hái sen trông rất thanh lịch.Năm đó, vào thời Nam-Tống, trấn Lăng hồ, phía Bắc trai thanh gái lịch du ngoạn rất nhiềụMột hôm, vào cận tiết Trung thu, dưới hồ sen có một chiếc ghe nhỏ chở năm người thiểu nữ lơ lửng trên dòng nước như mặt gương. Năm thiếu nữ ấy có ba người tuổi suýt soát nhau. Họ vừa hái sen vừa cất tiếng hát:Sen xa hồ, sen khô nhuỵ úaHồ không sen như dải lục đìa hiuTình đời mấy kẻ biết yêuGiữa lúc đó thì hai cô bé kia tuổi mới lên chín, chưa hiểu gì tình tứ của câu hát đó, và cũng không cần để ý, đưa tay chỉ một ông lão ngồi ở hồ sen cười nức nở.Một cô bé nói:-Kìa, đến hôm nay ông l\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !#&()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXY_abcdefghijklmnopqrstuvwxyz|¡ÀÁÂÃÈÊÓÔÕÙÚÛÝàáâãèéêìíïòóôõøùúûýĂăĐđĩũƠơƯưẠạẢảẤấẦầẩẫẬậẮắằẳẵặẹẻẽẾếỀềểỄễỆệỉịỌọỏỐốỒồổỗỘộỚớỜờỞởỡỢợụỦủứừỬửữựỳỵỷỹ“”\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68, 2, 76, 68, 71, 65, 57]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from characters to intergers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3815622]) torch.int64\n",
      "tensor([ 1, 38, 62, 66,  2, 31, 74, 67, 60,  1])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset and store it into a torch.tensor\n",
    "\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3434059, 381563)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split up the data into train and validation sets\n",
    "n= int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 38, 62, 66,  2, 31, 74, 67, 60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([ 1, 38, 62, 66,  2, 31, 74, 67])\n",
      "y=tensor([38, 62, 66,  2, 31, 74, 67, 60])\n",
      "when input is tensor([1]) the target: 38\n",
      "when input is tensor([ 1, 38]) the target: 62\n",
      "when input is tensor([ 1, 38, 62]) the target: 66\n",
      "when input is tensor([ 1, 38, 62, 66]) the target: 2\n",
      "when input is tensor([ 1, 38, 62, 66,  2]) the target: 31\n",
      "when input is tensor([ 1, 38, 62, 66,  2, 31]) the target: 74\n",
      "when input is tensor([ 1, 38, 62, 66,  2, 31, 74]) the target: 67\n",
      "when input is tensor([ 1, 38, 62, 66,  2, 31, 74, 67]) the target: 60\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print(f\"{x=}\")\n",
    "print(f\"{y=}\")\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8])\n",
      "tensor([[  2,  60,  97,  67,   2,  56, 160,  73],\n",
      "        [  2,  57, 129,  74,   2,  61,  62, 153],\n",
      "        [  2, 117, 160,  73,   2,  67,  61,  95],\n",
      "        [ 56, 106,   2,  73,  61, 135,  73,   2]])\n",
      "target:  torch.Size([4, 8])\n",
      "tensor([[ 60,  97,  67,   2,  56, 160,  73,   2],\n",
      "        [ 57, 129,  74,   2,  61,  62, 153,  74],\n",
      "        [117, 160,  73,   2,  67,  61,  95,   2],\n",
      "        [106,   2,  73,  61, 135,  73,   2,  56]])\n",
      "----\n",
      "when input is [2] the target: 60\n",
      "when input is [2, 60] the target: 97\n",
      "when input is [2, 60, 97] the target: 67\n",
      "when input is [2, 60, 97, 67] the target: 2\n",
      "when input is [2, 60, 97, 67, 2] the target: 56\n",
      "when input is [2, 60, 97, 67, 2, 56] the target: 160\n",
      "when input is [2, 60, 97, 67, 2, 56, 160] the target: 73\n",
      "when input is [2, 60, 97, 67, 2, 56, 160, 73] the target: 2\n",
      "when input is [2] the target: 57\n",
      "when input is [2, 57] the target: 129\n",
      "when input is [2, 57, 129] the target: 74\n",
      "when input is [2, 57, 129, 74] the target: 2\n",
      "when input is [2, 57, 129, 74, 2] the target: 61\n",
      "when input is [2, 57, 129, 74, 2, 61] the target: 62\n",
      "when input is [2, 57, 129, 74, 2, 61, 62] the target: 153\n",
      "when input is [2, 57, 129, 74, 2, 61, 62, 153] the target: 74\n",
      "when input is [2] the target: 117\n",
      "when input is [2, 117] the target: 160\n",
      "when input is [2, 117, 160] the target: 73\n",
      "when input is [2, 117, 160, 73] the target: 2\n",
      "when input is [2, 117, 160, 73, 2] the target: 67\n",
      "when input is [2, 117, 160, 73, 2, 67] the target: 61\n",
      "when input is [2, 117, 160, 73, 2, 67, 61] the target: 95\n",
      "when input is [2, 117, 160, 73, 2, 67, 61, 95] the target: 2\n",
      "when input is [56] the target: 106\n",
      "when input is [56, 106] the target: 2\n",
      "when input is [56, 106, 2] the target: 73\n",
      "when input is [56, 106, 2, 73] the target: 61\n",
      "when input is [56, 106, 2, 73, 61] the target: 135\n",
      "when input is [56, 106, 2, 73, 61, 135] the target: 73\n",
      "when input is [56, 106, 2, 73, 61, 135, 73] the target: 2\n",
      "when input is [56, 106, 2, 73, 61, 135, 73, 2] the target: 56\n"
     ]
    }
   ],
   "source": [
    "# Build dataset with block_size and batch_size\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  # generate small batch of data of input x and target y\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,)) # generate random integer from 0 to len(data) - block_size, output shape will be (batch_size, )\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ', xb.shape)\n",
    "print(xb)\n",
    "print('target: ', yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "  for t in range(block_size): # time dimension\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,  60,  97,  67,   2,  56, 160,  73],\n",
      "        [  2,  57, 129,  74,   2,  61,  62, 153],\n",
      "        [  2, 117, 160,  73,   2,  67,  61,  95],\n",
      "        [ 56, 106,   2,  73,  61, 135,  73,   2]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # input for transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bigram LM using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 191])\n",
      "tensor(5.8384, grad_fn=<NllLossBackward0>)\n",
      "\t|ÚéY/ỉĩHWôỬIFâôỳôÚịYó(kÛqøẳẻỜzặÊYĂ;ẵdịỷék_ÊợƠịiềẠú7ậIwj“ƯUnFờĂ&ỵR+ữq\tữ;7ÔãỀỳSÙ\n",
      "/ẾrôgẾÙ”Vacẵ(PẢAấy8Ề“\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Start with torch.zeros((1, 1) = [[1]] => start with \\t\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2647323608398438\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100_000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(337)\n",
    "B, T, C = 4, 8, 2 # Batch, Time, Channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3601, -0.0852],\n",
       "        [ 0.6640,  0.6716],\n",
       "        [ 0.8421,  1.6018],\n",
       "        [-1.4630,  0.0556],\n",
       "        [-1.0694,  1.0708],\n",
       "        [-0.5575, -0.4001],\n",
       "        [-0.3976, -0.5358],\n",
       "        [ 0.6464, -1.2248]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách này rất kém hiệu quả\n",
    "\n",
    "=> Trick: Sử dụng phép nhâ ma trận\n",
    "\n",
    "version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) # wei is short for weight\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3: Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "position_embedding_tablee = nn.Embedding(block_size, n_embed)\n",
    "pos_emb = position_embedding_tablee(torch.arange(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3067e-02, -1.6047e+00,  1.7878e+00, -4.7805e-01, -2.4286e-01,\n",
       "         -9.3416e-01, -2.4826e-01, -1.2082e+00, -7.6884e-01,  7.6245e-01,\n",
       "         -1.5673e+00, -2.3945e-01,  2.3228e+00, -9.6337e-01,  2.0024e+00,\n",
       "          4.6643e-01,  8.0080e-01,  1.6806e+00,  3.5586e-01, -6.8662e-01,\n",
       "         -4.9336e-01,  2.4149e-01, -1.1109e+00,  9.1546e-02, -2.5158e-01,\n",
       "          8.5986e-01, -3.0973e-01, -3.9571e-01,  8.0341e-01, -6.2160e-01,\n",
       "         -5.9200e-01, -6.3074e-02],\n",
       "        [ 3.0572e-01, -7.7459e-01,  3.4912e-02,  3.2110e-01,  1.5736e+00,\n",
       "         -8.4547e-01,  1.3123e+00,  6.8716e-01, -1.2347e+00, -4.8791e-01,\n",
       "         -1.4181e+00,  8.9627e-01,  4.9905e-02,  2.2667e+00,  1.1790e+00,\n",
       "         -4.3445e-01, -8.1401e-01, -7.3599e-01, -8.3712e-01, -9.2239e-01,\n",
       "          1.8113e+00,  1.6056e-01,  3.6725e-01,  1.7541e-01, -1.1845e+00,\n",
       "          1.3835e+00, -1.2024e+00,  7.0781e-01, -1.0759e+00,  5.3565e-01,\n",
       "          1.1754e+00,  5.6117e-01],\n",
       "        [-1.0546e+00,  1.2780e+00,  1.4534e-01,  2.3105e-01,  8.6540e-03,\n",
       "         -1.4229e-01,  1.9707e-01, -1.1441e+00, -2.2064e+00, -7.5080e-01,\n",
       "          2.8140e+00,  3.5979e-01, -8.9808e-02,  4.5844e-01, -5.6444e-01,\n",
       "          1.0563e+00,  1.1412e+00,  5.1644e-02,  7.2811e-01, -7.1064e-01,\n",
       "         -6.0207e-01,  9.6045e-01,  4.0481e-01, -1.3543e+00,  1.3347e+00,\n",
       "          4.8354e-01, -1.9756e-01,  1.2683e+00,  1.2243e+00,  9.8117e-02,\n",
       "          1.7423e+00, -1.3527e+00],\n",
       "        [ 1.0669e+00, -4.5015e-01, -6.7875e-01,  5.7432e-01,  1.8775e-01,\n",
       "         -3.5762e-01, -3.1651e-01,  5.8863e-01, -1.3109e-03, -3.0360e-01,\n",
       "         -9.8644e-01,  1.2330e-01,  3.4987e-01,  6.1728e-01, -1.6933e-01,\n",
       "          2.3323e-01, -3.8907e-01,  5.2792e-01,  1.0311e+00, -7.0477e-01,\n",
       "          1.0131e+00, -3.3082e-01,  5.1769e-01,  3.8778e-01,  7.1997e-01,\n",
       "          4.1141e-01, -5.7332e-01,  5.0686e-01, -4.7521e-01, -4.9203e-01,\n",
       "          2.7037e-01, -5.6282e-01],\n",
       "        [ 6.5474e-01,  5.7600e-01, -3.6091e-01, -6.0590e-02,  7.3255e-02,\n",
       "          8.1865e-01,  1.4805e+00,  3.4493e-01, -6.8665e-01,  6.3681e-01,\n",
       "          2.1755e-01, -4.6655e-02, -1.4335e+00, -5.6653e-01, -4.2528e-01,\n",
       "          2.6252e-01, -7.3280e-01,  1.0430e-01,  1.0414e+00, -3.9973e-01,\n",
       "         -2.2933e+00,  4.9756e-01, -4.2572e-01, -1.3371e+00, -1.1955e+00,\n",
       "          8.1234e-01, -3.0628e-01, -3.3016e-01, -9.8080e-01,  1.9473e-01,\n",
       "         -1.6535e+00,  6.8142e-01],\n",
       "        [ 1.7482e-01, -1.0939e+00,  9.6334e-01, -3.0953e-01,  5.7120e-01,\n",
       "          1.1179e+00, -1.2956e+00,  5.0276e-02,  7.7552e-01,  2.0265e+00,\n",
       "          9.8121e-01, -6.4012e-01, -4.9084e-01,  2.0801e-01, -1.1586e+00,\n",
       "         -9.6366e-01, -1.1360e+00, -5.2260e-01,  7.1654e-01,  1.5335e+00,\n",
       "         -1.4510e+00, -7.8614e-01, -9.5632e-01, -1.2476e+00,  7.0427e-01,\n",
       "          7.0988e-01, -1.5326e+00, -7.2513e-01,  4.6640e-01,  6.6672e-01,\n",
       "         -4.3871e-02,  2.3681e-01],\n",
       "        [ 4.7877e-01,  1.3537e+00, -1.5933e-01, -4.2494e-01,  9.4423e-01,\n",
       "         -1.8493e-01,  1.0608e+00,  2.0830e-01,  1.3065e+00,  4.5983e-01,\n",
       "          2.6178e-01, -7.5993e-01, -2.0461e+00, -1.5295e+00,  4.0487e-01,\n",
       "          6.3188e-01, -1.4829e-01, -2.3184e+00,  1.3032e+00,  4.8787e-01,\n",
       "          1.1340e+00, -3.5556e-01,  3.6183e-01,  1.9993e+00,  1.0350e+00,\n",
       "          1.6896e+00,  2.1274e-02, -8.2927e-01, -1.0809e+00, -7.8385e-01,\n",
       "          5.0710e-01,  8.2078e-02],\n",
       "        [ 3.9991e-01,  1.9892e+00, -4.6113e-01, -6.3885e-02, -1.3667e+00,\n",
       "          3.2982e-01, -9.8271e-01,  3.0177e-01,  1.9316e-01,  4.0967e-01,\n",
       "         -1.5754e+00,  2.2508e+00,  1.0012e+00,  1.3642e+00,  6.3332e-01,\n",
       "          4.0500e-01, -3.5325e-01,  1.4639e+00,  1.7290e-01,  1.0514e+00,\n",
       "          7.4915e-03, -7.7365e-02,  6.4269e-01,  5.7425e-01,  5.0579e-01,\n",
       "          2.2245e-01, -9.1432e-01,  1.4840e+00, -9.1091e-01, -5.2910e-01,\n",
       "         -8.0515e-01,  5.1580e-01]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention\n",
    "\n",
    "Note:\n",
    "\n",
    "- KEY: là những gì mình có\n",
    "- QUERY: là những gì mình đang tìm kiếm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C) # ([4, 8, 32])\n",
    "\n",
    "# let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # Chỉ transpose 2 chiều cuối, giữ nguyên batch, output: (B, T, T) \n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# out = wei @ x\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape # [4, 8, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "- Attention là một **communication mechanism**. Có thể được xem như các nút trong một đồ thị có hướng nhìn vào nhau và tổng hợp thông tin bằng tổng trọng số từ tất cả các nút trỏ đến chúng, với trọng số phụ thuộc vào dữ liệu.\n",
    "- Không có khái niệm về không gian. Attention chỉ đơn giản là hoạt động trên một tập hợp các vector. Đây là lý do tại sao chúng ta cần mã hóa vị trí (positionally encode) các token.\n",
    "- Mỗi ví dụ trên chiều batch được xử lý hoàn toàn độc lập và không bao giờ \"nói chuyện\" với nhau.\n",
    "- Trong một khối attention \"encoder\", chỉ cần xóa dòng duy nhất thực hiện masking với `tril`, cho phép tất cả các token giao tiếp. Khối này được gọi là khối attention \"decoder\" vì nó có masking hình tam giác và thường được sử dụng trong các thiết lập tự hồi quy (autoregressive), như mô hình hóa ngôn ngữ.\n",
    "- \"self-attention\" chỉ có nghĩa là các key và value được tạo ra từ cùng một nguồn với query. Trong \"cross-attention\", các query vẫn được tạo ra từ x, nhưng các key và value đến từ một nguồn bên ngoài khác (ví dụ: một mô-đun encoder).\n",
    "- Attention \"Scaled\" chia thêm `wei` cho 1/sqrt(head_size). Điều này làm cho khi đầu vào Q, K có phương sai đơn vị, wei cũng sẽ có phương sai đơn vị và Softmax sẽ vẫn khuếch tán và không bị bão hòa quá nhiều. Minh họa bên dưới.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() # Phương sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4690)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
