{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL (Remembering Conversation History): Adding Memory\n",
    "\n",
    "H∆∞·ªõng d·∫´n n√†y minh h·ªça c√°ch th√™m b·ªô nh·ªõ (memory) v√†o c√°c chu·ªói (chains) t√πy √Ω b·∫±ng c√°ch s·ª≠ d·ª•ng `LCEL`.\n",
    "\n",
    "`LangChain Expression Language (LCEL)` √°p d·ª•ng ph∆∞∆°ng ph√°p khai b√°o (declarative approach) ƒë·ªÉ x√¢y d·ª±ng c√°c `Runnables` m·ªõi t·ª´ c√°c `Runnables` hi·ªán c√≥. ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt v·ªÅ LCEL, vui l√≤ng tham kh·∫£o c√°c T√†i li·ªáu tham kh·∫£o (References) b√™n d∆∞·ªõi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model and Prompt\n",
    "\n",
    "Now, let's start to initialize the model and the prompt we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# Generate a conversational prompt. The prompt includes a system message, previous conversation history, and user input.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Memory\n",
    "\n",
    "Create a `ConversationBufferMemory` to store conversation history.\n",
    "\n",
    "- `return_messages` : When set to **True**, it returns `HumanMessage` and `AIMessage` objects.\n",
    "- `memory_key`: The key that will be substituted into the Chain's **prompt** later. This can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_813661/412775565.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "# Create a ConversationBufferMemory and enable the message return feature.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `RunnablePassthrough.assign` to assign the result of the `memory.load_memory_variables` function to the `chat_history` variable, and extract the value corresponding to the `chat_history` key from this result.\n",
    "\n",
    "Hold on a second! What is...\n",
    "\n",
    "### `RunnablePassthrough`? `RunnableLambda`?\n",
    "\n",
    "To put it simply, `RunnablePassthrough` provides the functionality to pass through data as is, <br>\n",
    "while `RunnableLambda` provides the functionality to execute user-defined functions.\n",
    "\n",
    "When you call `RunnablePassthrough` alone, it simply passes the input as received. <br>\n",
    "However, when you use `RunnablePassthrough.assign`, it delivers the input combined with additional arguments provided to the function.\n",
    "\n",
    "Let's look at the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi', 'chat_history': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables)\n",
    "    | itemgetter(\"chat_history\")  # itemgetter's input as same as memory_key.\n",
    ")\n",
    "\n",
    "runnable.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `RunnablePassthrough.assign` is used, the returned value is a combination of the input and the additional arguments provided to the function.\n",
    "\n",
    "In this case, the key of the additional argument is `chat_history`. The value corresponds to the part of the result of `memory.load_memory_variables` executed through `RunnableLambda` that is extracted by `itemgetter` using the `chat_history` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Memory to Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Heeah! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Using the invoke method of the chain object, a response to the input is generated.\n",
    "response = chain.invoke({\"input\": \"Nice to see you. My name is Heeah.\"})\n",
    "print(response.content)  # The generated response will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Nice to see you. My name is Heeah.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Heeah! How can I assist you today?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input data and response content are saved to the memory.\n",
    "# Here, it is 'Heeah', but try inserting your name!\n",
    "memory.save_context(\n",
    "    {\"human\": \"Nice to see you. My name is Heeah.\"}, {\"ai\": response.content}\n",
    ")\n",
    "\n",
    "# The saved conversation history will be printed.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember your name, Heeah. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"Do you remember my name?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Implementation of a Custom `ConversationChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initial setup of LLM and prompt, memory as done above.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "# If you want to use the summary memory that you learned in Chapter 6:\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=llm, return_messages=True, memory_key=\"chat_history\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Let's build our own ConversationChain!\n",
    "class MyConversationChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory, input_key=\"input\"):\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.input_key = input_key\n",
    "\n",
    "        # Let's try chaining using LCEL!\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables)\n",
    "                | itemgetter(memory.memory_key)\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({self.input_key: query})\n",
    "        self.memory.save_context(\n",
    "            inputs={\"human\": query}, outputs={\"ai\": answer}\n",
    "        )  # Store the conversation history directly in the memory.\n",
    "        return answer\n",
    "\n",
    "\n",
    "conversation_chain = MyConversationChain(llm, prompt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aye aye, matey! I be ready to sail the high seas and answer in pirate style for ye, Heeah! What be yer next command? Arrr!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\n",
    "    \"Hello, my name is Heeah. From now on, you are a brave pirate! You must answer in pirate style, understood?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Me favorite thing be the thrill of adventure on the open sea, the salty breeze in me hair, and the treasure huntin' that comes with bein' a pirate! What be yer favorite thing, me hearty? Arrr!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"Good. What's your favorite thing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aye, me heart be glad to hear that chattin' with me be yer favorite thing, Heeah! I be rememberin' yer name, fear not. What be yer next question or request, me matey? Arrr!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\n",
    "    \"My favorite thing is chatting with you! By the way, do you remember my name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Apologies, Captain Heeah! I be adjustin' me tone and showin' ye the proper respect. How may I be of service to ye, Captain? Arrr!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\n",
    "    \"I am the captain of this ship. Your tone is excessively familiar and disrespectful!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we managed to throw him off a bit at the end, we were able to confirm that he remembered my name until the last moment.<br>\n",
    "He is indeed a remarkable pirate!üè¥‚Äç‚ò†Ô∏è‚öì\n",
    "\n",
    "At any rate, the journey we have shared so far, as stored in the memory, is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, my name is Heeah. From now on, you are a brave pirate! You must answer in pirate style, understood?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Aye aye, matey! I be ready to sail the high seas and answer in pirate style for ye, Heeah! What be yer next command? Arrr!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Good. What's your favorite thing?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Me favorite thing be the thrill of adventure on the open sea, the salty breeze in me hair, and the treasure huntin' that comes with bein' a pirate! What be yer favorite thing, me hearty? Arrr!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My favorite thing is chatting with you! By the way, do you remember my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Aye, me heart be glad to hear that chattin' with me be yer favorite thing, Heeah! I be rememberin' yer name, fear not. What be yer next question or request, me matey? Arrr!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am the captain of this ship. Your tone is excessively familiar and disrespectful!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Apologies, Captain Heeah! I be adjustin' me tone and showin' ye the proper respect. How may I be of service to ye, Captain? Arrr!\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
