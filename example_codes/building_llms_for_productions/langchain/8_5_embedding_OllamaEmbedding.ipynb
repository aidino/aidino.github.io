{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Embeddings With Langchain\n",
    "\n",
    "Hướng dẫn này bao gồm cách thực hiện `Text Embedding` sử dụng `Ollama` và `Langchain`.\n",
    "\n",
    "`Ollama` là một dự án mã nguồn mở cho phép bạn dễ dàng phục vụ các mô hình cục bộ.\n",
    "\n",
    "Trong hướng dẫn này, chúng ta sẽ tạo một ví dụ đơn giản để đo độ tương đồng giữa các `Documents` và một `Query` đầu vào sử dụng `Ollama` và `Langchain`.\n",
    "\n",
    "![example](./assets/example-flow-ollama-embedding-cal-similarity.png)\n",
    "\n",
    "```bash\n",
    "pip install langchain-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Load and Embedding\n",
    "\n",
    "Now that you have downloaded the model, let's load the model you downloaded and proceed with the embedding.\n",
    "\n",
    "First, define `Query` and `Documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "q = \"Please tell me more about LangChain.\"\n",
    "\n",
    "# Documents for Text Embedding\n",
    "docs = [\n",
    "    \"Hi, nice to meet you.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models.\",\n",
    "    \"The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\",\n",
    "    \"LangChain simplifies the process of building applications with large-scale language models.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the embedding model downloaded with `Ollama` using `Langchain`.\n",
    "\n",
    "\n",
    "The `OllamaEmbeddings` class in `langchain_community/embeddings.py` will be removed in `langchain-community` version 1.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_215802/35084152.py:5: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embeddings = OllamaEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Load Embedding Model : Legacy\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Serving and load the desired embedding model.\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",  # model=<model-name>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this tutorial, we used the `OllamaEmbeddings` class from `langchain-ollama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding Model : Latest\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Serving and load the desired embedding model.\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",  # model=<model-name>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension Output: 768\n"
     ]
    }
   ],
   "source": [
    "# Embedding Query\n",
    "embedded_query = ollama_embeddings.embed_query(q)\n",
    "\n",
    "# Embedding Documents\n",
    "embedded_docs = ollama_embeddings.embed_documents(docs)\n",
    "\n",
    "print(f\"Embedding Dimension Output: {len(embedded_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The similarity calculation results\n",
    "\n",
    "Let's use the vector values of the query and documents obtained earlier to calculate the similarity.\n",
    "\n",
    "In this tutorial, we will use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the `Query` and the `Documents`.\n",
    "\n",
    "Using the `Sklearn` library in Python, you can easily calculate **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Tell me about LangChain.\n",
      "====================================\n",
      "[0] similarity: 0.775 | The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
      "\n",
      "[1] similarity: 0.749 | LangChain simplifies the process of building applications with large language models.\n",
      "\n",
      "[2] similarity: 0.745 | LangChain simplifies the process of building applications with large-scale language models.\n",
      "\n",
      "[3] similarity: 0.399 | Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n",
      "[4] similarity: 0.398 | Hi, nice to meet you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "similarity = cosine_similarity([embedded_query], embedded_docs)\n",
    "\n",
    "# Sorting by Similarity in descending order\n",
    "sorted_idx = similarity.argsort()[0][::-1]\n",
    "\n",
    "# Output Result\n",
    "print(\"[Query] Tell me about LangChain.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] similarity: {similarity[0][idx]:.3f} | {docs[idx]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
