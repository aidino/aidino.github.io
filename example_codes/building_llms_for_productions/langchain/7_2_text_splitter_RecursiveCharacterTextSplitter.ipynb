{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "Hướng dẫn này giải thích cách sử dụng `RecursiveCharacterTextSplitter`, phương pháp được khuyến nghị để tách văn bản trong LangChain.\n",
    "\n",
    "`RecursiveCharacterTextSplitter` hoạt động bằng cách lấy một danh sách các ký tự và cố gắng chia văn bản thành các phần nhỏ hơn dựa trên danh sách đó. Nó tiếp tục chia cho đến khi các phần đủ nhỏ.\n",
    "\n",
    "Theo mặc định, danh sách ký tự là **['\\\\n\\\\n', '\\\\n', ' ', '']**, có nghĩa là nó chia đệ quy theo thứ tự sau: **paragraph** -> **sentence** -> **word**. Điều này ưu tiên giữ các đoạn văn, sau đó là câu, sau đó là từ cùng nhau càng nhiều càng tốt, vì chúng được coi là các đơn vị liên quan về mặt ngữ nghĩa nhất.\n",
    "\n",
    "Dưới đây là tóm tắt cách thức hoạt động của nó:\n",
    "\n",
    "1.  Việc tách được thực hiện bởi một danh sách các ký tự (**['\\\\n\\\\n', '\\\\n', ' ', '']**).\n",
    "2.  Kích thước chunk được đo bằng số lượng ký tự.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of RecursiveCharacterTextSplitter\n",
    "\n",
    "This example demonstrates how to use the `RecursiveCharacterTextSplitter` to split text into smaller chunks.\n",
    "1. Open the text file `appendix-keywords.txt` and read its contents and store this text in a variable named `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the appendix-keywords.txt file to create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Reads the contents of the file and stores them in the file variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display some of the content read from the `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words\n"
     ]
    }
   ],
   "source": [
    "# Output the top 500 characters read from the file.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, create a `RecursiveCharacterTextSplitter` with the following parameters:\n",
    "\n",
    "- `chunk_size` = 250 (limits each chunk to 250 characters)\n",
    "- `chunk_overlap` = 50 (allows 50 characters of overlap between chunks)\n",
    "- `length_function` = `len()` (specifies that built-in `len()` function for length calculation)\n",
    "- `is_separator_regex` = `False` (disables regular expression separators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set the chunk size to very small. These settings are for illustrative purposes only.\n",
    "    chunk_size=250,\n",
    "    # Sets the number of overlapping characters between chunks.\n",
    "    chunk_overlap=50,\n",
    "    # Specifies a function to calculate the length of the string.\n",
    "    length_function=len,\n",
    "    # Sets whether to use regular expressions as delimiters.\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the `text_splitter` to split the text stored in the `file` variable into a list of `Document` objects. This list will be stored in a variable called `texts`.\n",
    "5. Print the first and second documents using `print(texts[0])` and `print(texts[1])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search'\n",
      "============================================================\n",
      "page_content='Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.'\n"
     ]
    }
   ],
   "source": [
    "# Split the file text into documents using text_splitter.\n",
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])  # Outputs the first document in the split document.\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])  # Output the second document of the split document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `text_splitter.split_text()` function to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search',\n",
       " 'Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits the text and returns the first two elements of the split text.\n",
    "text_splitter.split_text(file)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
