{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output Parser\n",
    "\n",
    "`StructuredOutputParser` là một công cụ giá trị để định dạng phản hồi của Mô hình Ngôn ngữ Lớn (LLM) thành cấu trúc từ điển, cho phép trả về nhiều trường dưới dạng cặp key/value.\n",
    "Trong khi Pydantic và JSON parsers cung cấp các khả năng mạnh mẽ, `StructuredOutputParser` đặc biệt hiệu quả đối với các mô hình ít mạnh mẽ hơn, chẳng hạn như các mô hình cục bộ với ít tham số hơn. Nó đặc biệt có lợi cho các mô hình có trí tuệ thấp hơn so với các mô hình tiên tiến như GPT hoặc Claude.\n",
    "Bằng cách sử dụng `StructuredOutputParser`, các nhà phát triển có thể duy trì tính toàn vẹn và nhất quán của dữ liệu trên nhiều ứng dụng LLM khác nhau, ngay cả khi hoạt động với các mô hình có số lượng tham số giảm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Structured Output Parser\n",
    "\n",
    "### Using ResponseSchema with StructuredOutputParser\n",
    "*   Define a response schema using the `ResponseSchema` class to include the answer to the user's question and a `description` of the source (website) used.\n",
    "\n",
    "*   Initialize `StructuredOutputParser` with `response_schemas` to structure the output according to the defined response schema.\n",
    "\n",
    "**[Note]**\n",
    "When using local models, Pydantic parsers may frequently fail to work properly. In such cases, using `StructuredOutputParser` can be a good alternative solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Response to the user's question\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"Answer to the user's question\"),\n",
    "    ResponseSchema(\n",
    "        name=\"source\",\n",
    "        description=\"The `source` used to answer the user's question, which should be a `website URL`.\",\n",
    "    ),\n",
    "]\n",
    "# Initialize the structured output parser based on the response schemas\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Response Schemas into Prompts \n",
    "\n",
    "Create a `PromptTemplate` to format user questions and embed parsing instructions for structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Parse the format instructions.\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    # Set up the template to answer the user's question as best as possible.\n",
    "    template=\"answer the user's question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    # Use 'question' as the input variable.\n",
    "    input_variables=[\"question\"],\n",
    "    # Use 'format_instructions' as a partial variable.\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The largest desert in the world is the Antarctic Desert.',\n",
       " 'source': 'https://www.britannica.com/place/desert'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"question\": \"What is the largest desert in the world?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Streamed Outputs\n",
    "\n",
    "Use the `chain.stream` method to receive a streaming response to the `question` , \"How many players are on a soccer team?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'A standard soccer team consists of 11 players on the field at a time.', 'source': 'https://www.fifa.com/who-we-are/news/what-are-the-rules-of-soccer'}\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"How many players are on a soccer team?\"}):\n",
    "    # Stream the output\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
