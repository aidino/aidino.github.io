{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Loader\n",
    "\n",
    "Hướng dẫn này bao gồm các phương pháp xử lý PDF khác nhau sử dụng LangChain và các thư viện PDF phổ biến.\n",
    "\n",
    "Xử lý PDF là điều cần thiết để trích xuất và phân tích dữ liệu văn bản từ tài liệu PDF.\n",
    "\n",
    "Trong hướng dẫn này, chúng ta sẽ khám phá các trình tải (loaders) PDF khác nhau và khả năng của chúng khi làm việc với khung xử lý tài liệu (document processing framework) của LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load PDFs\n",
    "\n",
    "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), một định dạng tệp được tiêu chuẩn hóa bởi ISO 32000, được Adobe phát triển vào năm 1992 để trình bày tài liệu, bao gồm định dạng văn bản và hình ảnh theo cách độc lập với phần mềm ứng dụng, phần cứng và hệ điều hành.\n",
    "\n",
    "Hướng dẫn này bao gồm cách tải tài liệu PDF vào định dạng [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) của LangChain. Định dạng này sẽ được sử dụng ở các bước tiếp theo.\n",
    "\n",
    "LangChain tích hợp với nhiều trình phân tích cú pháp (parsers) PDF khác nhau. Một số đơn giản và tương đối cấp thấp, trong khi những trình khác hỗ trợ OCR và xử lý hình ảnh hoặc thực hiện phân tích bố cục tài liệu nâng cao.\n",
    "\n",
    "Sự lựa chọn phù hợp phụ thuộc vào ứng dụng của bạn.\n",
    "\n",
    "Chúng tôi sẽ trình diễn các phương pháp này trên một [tệp mẫu](https://github.com/langchain-ai/langchain/blob/master/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf).\n",
    "Tải xuống tệp mẫu và sao chép nó vào thư mục dữ liệu của bạn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data/layout-parser-paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metadata(docs):\n",
    "    if docs:\n",
    "        print(\"[metadata]\")\n",
    "        print(list(docs[0].metadata.keys()))\n",
    "        print(\"\\n[examples]\")\n",
    "        max_key_length = max(len(k) for k in docs[0].metadata.keys())\n",
    "        for k, v in docs[0].metadata.items():\n",
    "            print(f\"{k:<{max_key_length}} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDF\n",
    "\n",
    "[PyPDF](https://github.com/py-pdf/pypdf) là một trong những thư viện Python được sử dụng rộng rãi nhất để xử lý PDF.\n",
    "\n",
    "Ở đây, chúng ta sử dụng PyPDF để tải PDF dưới dạng danh sách các đối tượng Document.\n",
    "\n",
    "[`PyPDFLoader`]([https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)) của LangChain tích hợp với PyPDF để phân tích cú pháp các tài liệu PDF thành các đối tượng Document của LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA 11\n",
      "focuses on precision, eﬃciency, and robustness. The target documents may have\n",
      "complicated structures, and may require training multiple layout detection models\n",
      "to achieve the optimal accuracy. Light-weight pipelines are built for relatively\n",
      "simple d\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize the PDF loader\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "\n",
    "# Load data into Document objects\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the contents of the document\n",
    "print(docs[10].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['producer', 'creator', 'creationdate', 'author', 'keywords', 'moddate', 'ptex.fullbanner', 'subject', 'title', 'trapped', 'source', 'total_pages', 'page', 'page_label']\n",
      "\n",
      "[examples]\n",
      "producer        : pdfTeX-1.40.21\n",
      "creator         : LaTeX with hyperref\n",
      "creationdate    : 2021-06-22T01:27:10+00:00\n",
      "author          : \n",
      "keywords        : \n",
      "moddate         : 2021-06-22T01:27:10+00:00\n",
      "ptex.fullbanner : This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "subject         : \n",
      "title           : \n",
      "trapped         : /False\n",
      "source          : data/layout-parser-paper.pdf\n",
      "total_pages     : 16\n",
      "page            : 0\n",
      "page_label      : 1\n"
     ]
    }
   ],
   "source": [
    "# output metadata\n",
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_and_split()` method allows customizing how documents are chunked by passing a text splitter object, making it more flexible for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1 (\u0000 ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load Documents and split into chunks. Chunks are returned as Documents.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPDF(OCR)\n",
    "\n",
    "Some PDFs contain text images within scanned documents or pictures. You can also use the `rapidocr-onnxruntime` package to extract text from images.\n",
    "\n",
    "```bash\n",
    "pip install rapidocr-onnxruntime\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA 5\n",
      "Table 1: Current layout detection models in the LayoutParser model zoo\n",
      "Dataset Base Model1 Large ModelNotes\n",
      "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
      "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
      "Newspaper\n"
     ]
    }
   ],
   "source": [
    "# Initialize PDF loader, enable image extraction option\n",
    "loader = PyPDFLoader(FILE_PATH, extract_images=True)\n",
    "\n",
    "# load PDF page\n",
    "docs = loader.load()\n",
    "\n",
    "# access page content\n",
    "print(docs[4].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['producer', 'creator', 'creationdate', 'author', 'keywords', 'moddate', 'ptex.fullbanner', 'subject', 'title', 'trapped', 'source', 'total_pages', 'page', 'page_label']\n",
      "\n",
      "[examples]\n",
      "producer        : pdfTeX-1.40.21\n",
      "creator         : LaTeX with hyperref\n",
      "creationdate    : 2021-06-22T01:27:10+00:00\n",
      "author          : \n",
      "keywords        : \n",
      "moddate         : 2021-06-22T01:27:10+00:00\n",
      "ptex.fullbanner : This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "subject         : \n",
      "title           : \n",
      "trapped         : /False\n",
      "source          : data/layout-parser-paper.pdf\n",
      "total_pages     : 16\n",
      "page            : 0\n",
      "page_label      : 1\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPDF Directory\n",
    "\n",
    "Import all PDF documents from directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# directory path\n",
    "loader = PyPDFDirectoryLoader(\"data\")\n",
    "\n",
    "# load documents\n",
    "docs = loader.load()\n",
    "\n",
    "# print the number of documents\n",
    "docs_len = len(docs)\n",
    "print(docs_len)\n",
    "\n",
    "# get document from a directory\n",
    "document = docs[docs_len - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Z. Shen et al.\n",
      "[23] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\n",
      "Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)\n",
      "[24] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\n",
      "T., Lin, Z., Gimelshein, N., An\n"
     ]
    }
   ],
   "source": [
    "# print the contents of the document\n",
    "print(document.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/layout-parser-paper.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}\n"
     ]
    }
   ],
   "source": [
    "print(document.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data/01-document-loader-sample.pdf', 'data/layout-parser-paper.pdf'}\n"
     ]
    }
   ],
   "source": [
    "source_set = set()\n",
    "for doc in docs:\n",
    "    source_set.add(doc.metadata[\"source\"])\n",
    "\n",
    "print(source_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMuPDF\n",
    "\n",
    "[PyMuPDF](https://github.com/pymupdf/PyMuPDF) được tối ưu hóa tốc độ và bao gồm siêu dữ liệu (metadata) chi tiết về PDF và các trang của nó. Nó trả về một document cho mỗi trang.\n",
    "\n",
    "[`PyMuPDFLoader`]([https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html)) của LangChain tích hợp với PyMuPDF để phân tích cú pháp các tài liệu PDF thành các đối tượng Document của LangChain.\n",
    "\n",
    "```bash\n",
    "pip install pymupdf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n",
      "11\n",
      "focuses on precision, eﬃciency, and robustness. The target documents may have\n",
      "complicated structures, and may require training multiple layout detection models\n",
      "to achieve the optimal accuracy. Light-weight pipelines are built for relatively\n",
      "simple d\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# create an instance of the PyMuPDF loader\n",
    "loader = PyMuPDFLoader(FILE_PATH)\n",
    "\n",
    "# load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[10].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['producer', 'creator', 'creationdate', 'source', 'file_path', 'total_pages', 'format', 'title', 'author', 'subject', 'keywords', 'moddate', 'trapped', 'modDate', 'creationDate', 'page']\n",
      "\n",
      "[examples]\n",
      "producer     : pdfTeX-1.40.21\n",
      "creator      : LaTeX with hyperref\n",
      "creationdate : 2021-06-22T01:27:10+00:00\n",
      "source       : data/layout-parser-paper.pdf\n",
      "file_path    : data/layout-parser-paper.pdf\n",
      "total_pages  : 16\n",
      "format       : PDF 1.5\n",
      "title        : \n",
      "author       : \n",
      "subject      : \n",
      "keywords     : \n",
      "moddate      : 2021-06-22T01:27:10+00:00\n",
      "trapped      : \n",
      "modDate      : D:20210622012710Z\n",
      "creationDate : D:20210622012710Z\n",
      "page         : 0\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured\n",
    "\n",
    "[Unstructured](https://docs.unstructured.io/welcome) là một thư viện mạnh mẽ được thiết kế để xử lý nhiều định dạng tài liệu phi cấu trúc và bán cấu trúc khác nhau. Nó vượt trội trong việc tự động xác định và phân loại các thành phần khác nhau trong tài liệu.\n",
    "Hiện tại hỗ trợ tải các tệp văn bản, PowerPoints, HTML, PDF, hình ảnh và hơn thế nữa.\n",
    "\n",
    "[`UnstructuredPDFLoader`]([https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html)) của LangChain tích hợp với Unstructured để phân tích cú pháp các tài liệu PDF thành các đối tượng Document của LangChain.\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-community unstructured\n",
    "pip install pi_heif\n",
    "pip install unstructured_inference\n",
    "pip install pdf2image\n",
    "pip install -U pdfminer.six\n",
    "pip install pytesseract\n",
    "pip install unstructured_pytesseract\n",
    "sudo apt install tesseract-ocr\n",
    "sudo apt install libtesseract-dev\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103.15348v2 [cs.CV] 21 Jun 2021\n",
      "\n",
      "arXiv\n",
      "\n",
      "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis\n",
      "\n",
      "Zejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li>\n",
      "\n",
      "1 Allen Institute for AI shannons@allenai.org ? Brown University\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# create an instance of UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(FILE_PATH)\n",
    "\n",
    "# load the data\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source']\n",
      "\n",
      "[examples]\n",
      "source : data/layout-parser-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Về mặt nội bộ, Unstructured tạo ra các \"**elements**\" khác nhau cho mỗi khối văn bản. Theo mặc định, chúng được kết hợp, nhưng có thể dễ dàng tách biệt bằng cách chỉ định `mode=\"elements\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103.15348v2 [cs.CV] 21 Jun 2021\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of UnstructuredPDFLoader (mode=\"elements”)\n",
    "loader = UnstructuredPDFLoader(FILE_PATH, mode=\"elements\")\n",
    "\n",
    "# load the data\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ListItem', 'NarrativeText', 'Title', 'UncategorizedText'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(doc.metadata[\"category\"] for doc in docs) # extract data categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id']\n",
      "\n",
      "[examples]\n",
      "source         : data/layout-parser-paper.pdf\n",
      "coordinates    : {'points': ((51.0, 599.0), (51.0, 1411.0), (96.0, 1411.0), (96.0, 599.0)), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}\n",
      "filetype       : application/pdf\n",
      "languages      : ['eng']\n",
      "last_modified  : 2025-03-11T08:50:59\n",
      "page_number    : 1\n",
      "file_directory : data\n",
      "filename       : layout-parser-paper.pdf\n",
      "category       : UncategorizedText\n",
      "element_id     : 717b88dc7738c0e45a97b66c00b98b3d\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDFium2\n",
    "\n",
    "LangChain's [`PyPDFium2Loader`](\n",
    "https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFium2Loader.html) integrates with [PyPDFium2](https://github.com/pypdfium2-team/pypdfium2) to parse PDF documents into LangChain Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Unified Toolkit for DL-Based DIA 11\n",
      "focuses on precision, efficiency, and robustness. The target documents may have\n",
      "complicated structures, and may require training multiple layout detection models\n",
      "to achieve the optimal accuracy. Light-weight pipelines are built for relatively\n",
      "simpl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dino/miniconda3/envs/langchain/lib/python3.10/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "# create an instance of the PyPDFium2 loader\n",
    "loader = PyPDFium2Loader(FILE_PATH)\n",
    "\n",
    "# load data\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[10].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lưu ý**: Khi sử dụng `PyPDFium2Loader`, bạn có thể nhận thấy các thông báo cảnh báo liên quan đến `get_text_range()`. Những cảnh báo này là một phần của hoạt động nội bộ của thư viện và không ảnh hưởng đến chức năng xử lý PDF. Bạn có thể tiếp tục an toàn với hướng dẫn mặc dù có những cảnh báo này, vì chúng là một phần bình thường của môi trường phát triển và không ảnh hưởng đến mục tiêu học tập.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['title', 'author', 'subject', 'keywords', 'creator', 'producer', 'creationdate', 'moddate', 'source', 'total_pages', 'page']\n",
      "\n",
      "[examples]\n",
      "title        : \n",
      "author       : \n",
      "subject      : \n",
      "keywords     : \n",
      "creator      : LaTeX with hyperref\n",
      "producer     : pdfTeX-1.40.21\n",
      "creationdate : 2021-06-22T01:27:10+00:00\n",
      "moddate      : 2021-06-22T01:27:10+00:00\n",
      "source       : data/layout-parser-paper.pdf\n",
      "total_pages  : 16\n",
      "page         : 0\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFMiner\n",
    "[PDFMiner](https://github.com/pdfminer/pdfminer.six) is a specialized Python library focused on text extraction and layout analysis from PDF documents.\n",
    "\n",
    "LangChain's [`PDFMinerLoader`](\n",
    "https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PDFMinerLoader.html) integrates with PDFMiner to parse PDF documents into LangChain Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "n\n",
      "u\n",
      "J\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "]\n",
      "\n",
      "V\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "2\n",
      "v\n",
      "8\n",
      "4\n",
      "3\n",
      "5\n",
      "1\n",
      ".\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "\n",
      "Zejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "\n",
      "1 Allen Institute for AI\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "# Create a PDFMiner loader instance\n",
    "loader = PDFMinerLoader(FILE_PATH)\n",
    "\n",
    "# load data\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['author', 'creationdate', 'creator', 'keywords', 'moddate', 'ptex.fullbanner', 'producer', 'subject', 'title', 'trapped', 'total_pages', 'source']\n",
      "\n",
      "[examples]\n",
      "author          : \n",
      "creationdate    : 2021-06-22T01:27:10+00:00\n",
      "creator         : LaTeX with hyperref\n",
      "keywords        : \n",
      "moddate         : 2021-06-22T01:27:10+00:00\n",
      "ptex.fullbanner : This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "producer        : pdfTeX-1.40.21\n",
      "subject         : \n",
      "title           : \n",
      "trapped         : False\n",
      "total_pages     : 16\n",
      "source          : data/layout-parser-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PDFMiner to generate HTML text\n",
    "\n",
    "This method allows you to parse the output HTML content through [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/) to get more structured and richer information about font size, page numbers, PDF header/footer, etc. which can help you semantically split the text into sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html\">\n",
      "</head><body>\n",
      "<span style=\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:792px;\"></span>\n",
      "<div style=\"position:absolute; top:50px;\"><a name=\"1\">Page 1</a></div>\n",
      "<div style=\"position:absolute; border\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "\n",
    "# create an instance of PDFMinerPDFasHTMLLoader\n",
    "loader = PDFMinerPDFasHTMLLoader(FILE_PATH)\n",
    "\n",
    "# load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source']\n",
      "\n",
      "[examples]\n",
      "source : data/layout-parser-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `beautifulsoup4`\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(docs[0].page_content, \"html.parser\") # initialize HTML parser\n",
    "content = soup.find_all(\"div\") # search for all div tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cur_fs = None\n",
    "cur_text = \"\"\n",
    "snippets = []  # collect all snippets of the same font size\n",
    "for c in content:\n",
    "    sp = c.find(\"span\")\n",
    "    if not sp:\n",
    "        continue\n",
    "    st = sp.get(\"style\")\n",
    "    if not st:\n",
    "        continue\n",
    "    fs = re.findall(\"font-size:(\\d+)px\", st)\n",
    "    if not fs:\n",
    "        continue\n",
    "    fs = int(fs[0])\n",
    "    if not cur_fs:\n",
    "        cur_fs = fs\n",
    "    if fs == cur_fs:\n",
    "        cur_text += c.text\n",
    "    else:\n",
    "        snippets.append((cur_text, cur_fs))\n",
    "        cur_fs = fs\n",
    "        cur_text = c.text\n",
    "snippets.append((cur_text, cur_fs))\n",
    "# Note: Possibility to add a strategy for removing duplicate snippets (since the header/footer of a PDF appears across multiple pages, \n",
    "# it can be considered duplicate information when found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Recently, various DL models and datasets have been developed for layout analysis\n",
      "tasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\n",
      "tation tasks on historical documents. Object detection-based methods like Faster\n",
      "R-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\n",
      "and detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\n",
      "been used in table detection [27]. However, these models are usually implemented\n",
      "individually and there is no uniﬁed framework to load and use such models.\n",
      "There has been a surge of interest in creating open-source tools for document\n",
      "image processing: a search of document image analysis in Github leads to 5M\n",
      "relevant code pieces 6; yet most of them rely on traditional rule-based methods\n",
      "or provide limited functionalities. The closest prior research to our work is the\n",
      "OCR-D project7, which also tries to build a complete toolkit for DIA. However,\n",
      "similar to the platform developed by Neudecker et al. [21], it is designed for\n",
      "analyzing historical documents, and provides no supports for recent DL models.\n",
      "The DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\n",
      "documents via analyzing the stored PDF data. Repositories like DeepLayout9\n",
      "and Detectron2-PubLayNet10 are individual deep learning models trained on\n",
      "layout analysis datasets without support for the full DIA pipeline. The Document\n",
      "Analysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\n",
      "aim to improve the reproducibility of DIA methods (or DL models), yet they\n",
      "are not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\n",
      "paddleOCR12 usually do not come with comprehensive functionalities for other\n",
      "DIA tasks like layout analysis.\n",
      "Recent years have also seen numerous eﬀorts to create libraries for promoting\n",
      "reproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [35],\n",
      "6 The number shown is obtained by specifying the search type as ‘code’.\n",
      "7 https://ocr-d.de/en/about\n",
      "8 https://github.com/BobLd/DocumentLayoutAnalysis\n",
      "9 https://github.com/leonlulu/DeepLayout\n",
      "10 https://github.com/hpanwar08/detectron2\n",
      "11 https://github.com/JaidedAI/EasyOCR\n",
      "12 https://github.com/PaddlePaddle/PaddleOCR\n",
      "4\n",
      "Z. Shen et al.\n",
      "Fig. 1: The overall architecture of LayoutParser. For an input document image,\n",
      "the core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\n",
      "detection, OCR, visualization, and storage, backed by a carefully designed layout\n",
      "data structure. LayoutParser also supports high level customization via eﬃcient\n",
      "layout annotation and model training functions. These improve model accuracy\n",
      "on the target samples. The community platform enables the easy sharing of DIA\n",
      "models and whole digitization pipelines to promote reusability and reproducibility.\n",
      "A collection of detailed documentation, tutorials and exemplar projects make\n",
      "LayoutParser easy to learn and use.\n",
      "AllenNLP [8] and transformers [34] have provided the community with complete\n",
      "DL-based support for developing and deploying models for general computer\n",
      "vision and natural language processing problems. LayoutParser, on the other\n",
      "hand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\n",
      "community platform inspired by established model hubs such as Torch Hub [23]\n",
      "and TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\n",
      "full document processing pipelines that are unique to DIA tasks.\n",
      "There have been a variety of document data collections to facilitate the\n",
      "development of DL models. Some examples include PRImA [3](magazine layouts),\n",
      "PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\n",
      "papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and\n",
      "HJDataset [31](historical Japanese document layouts). A spectrum of models\n",
      "trained on these datasets are currently available in the LayoutParser model zoo\n",
      "to support diﬀerent use cases.\n",
      "' metadata={'heading': '2 Related Work\\n', 'content_font': 9, 'heading_font': 11, 'source': 'data/layout-parser-paper.pdf'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "cur_idx = -1\n",
    "semantic_snippets = []\n",
    "# Assumption: headings have higher font size than their respective content\n",
    "for s in snippets:\n",
    "    # if current snippet's font size > previous section's heading => it is a new heading\n",
    "    if (\n",
    "        not semantic_snippets\n",
    "        or s[1] > semantic_snippets[cur_idx].metadata[\"heading_font\"]\n",
    "    ):\n",
    "        metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "        metadata.update(docs[0].metadata)\n",
    "        semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "        cur_idx += 1\n",
    "        continue\n",
    "\n",
    "    # if current snippet's font size <= previous section's content => content belongs to the same section (one can also create\n",
    "    if (\n",
    "        not semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        or s[1] <= semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "    ):\n",
    "        semantic_snippets[cur_idx].page_content += s[0]\n",
    "        semantic_snippets[cur_idx].metadata[\"content_font\"] = max(\n",
    "            s[1], semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # if current snippet's font size > previous section's content but less than previous section's heading than also make a new\n",
    "    metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "    metadata.update(docs[0].metadata)\n",
    "    semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "    cur_idx += 1\n",
    "\n",
    "print(semantic_snippets[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFPlumber\n",
    "\n",
    "[PDFPlumber](https://github.com/jsvine/pdfplumber) là một thư viện phân tích cú pháp PDF vượt trội trong việc trích xuất văn bản và bảng từ PDF.\n",
    "\n",
    "[`PDFPlumberLoader`]([https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PDFPlumberLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PDFPlumberLoader.html)) của LangChain tích hợp với PDFPlumber để phân tích cú pháp tài liệu PDF thành các đối tượng Document của LangChain.\n",
    "\n",
    "Giống như PyMuPDF, tài liệu đầu ra chứa siêu dữ liệu (metadata) chi tiết về PDF và các trang của nó, đồng thời trả về một tài liệu cho mỗi trang.\n",
    "\n",
    "```bash\n",
    "pip install pdfplumber\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Unified Toolkit for DL-Based DIA 11\n",
      "focuses on precision, efficiency, and robustness. The target documents may have\n",
      "complicatedstructures,andmayrequiretrainingmultiplelayoutdetectionmodels\n",
      "to achieve the optimal accuracy. Light-weight pipelines are built for relatively\n",
      "simple documen\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# create a PDF document loader instance\n",
    "loader = PDFPlumberLoader(FILE_PATH)\n",
    "\n",
    "# load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# access the first document data\n",
    "print(docs[10].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'file_path', 'page', 'total_pages', 'Author', 'CreationDate', 'Creator', 'Keywords', 'ModDate', 'PTEX.Fullbanner', 'Producer', 'Subject', 'Title', 'Trapped']\n",
      "\n",
      "[examples]\n",
      "source          : data/layout-parser-paper.pdf\n",
      "file_path       : data/layout-parser-paper.pdf\n",
      "page            : 0\n",
      "total_pages     : 16\n",
      "Author          : \n",
      "CreationDate    : D:20210622012710Z\n",
      "Creator         : LaTeX with hyperref\n",
      "Keywords        : \n",
      "ModDate         : D:20210622012710Z\n",
      "PTEX.Fullbanner : This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "Producer        : pdfTeX-1.40.21\n",
      "Subject         : \n",
      "Title           : \n",
      "Trapped         : False\n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
