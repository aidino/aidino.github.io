{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Text Splitting Methods in NLP\n",
    "\n",
    "\n",
    "Việc tách văn bản (text splitting) là một bước tiền xử lý quan trọng trong Xử lý Ngôn ngữ Tự nhiên (NLP). Hướng dẫn này bao gồm các phương pháp và công cụ tách văn bản khác nhau, khám phá ưu điểm, nhược điểm và các trường hợp sử dụng phù hợp của chúng.\n",
    "\n",
    "Các phương pháp chính để tách văn bản:\n",
    "\n",
    "1.  **Tách dựa trên Token (Token-based Splitting)**\n",
    "    -   Tiktoken: Bộ mã hóa BPE hiệu suất cao của OpenAI\n",
    "    -   Hugging Face tokenizers: Bộ mã hóa cho nhiều mô hình tiền huấn luyện khác nhau\n",
    "\n",
    "2.  **Tách dựa trên Câu (Sentence-based Splitting)**\n",
    "    -   SentenceTransformers: Tách văn bản trong khi duy trì tính mạch lạc ngữ nghĩa\n",
    "    -   NLTK: Tách câu và từ dựa trên xử lý ngôn ngữ tự nhiên\n",
    "    -   spaCy: Tách văn bản sử dụng các khả năng xử lý ngôn ngữ nâng cao\n",
    "\n",
    "3.  **Công cụ đặc thù ngôn ngữ (Language-specific Tools)**\n",
    "    -   KoNLPy: Công cụ tách chuyên dụng cho xử lý văn bản tiếng Hàn\n",
    "\n",
    "Mỗi công cụ có các đặc điểm và ưu điểm riêng:\n",
    "\n",
    "-   Tiktoken cung cấp tốc độ xử lý nhanh và khả năng tương thích với các mô hình OpenAI\n",
    "-   SentenceTransformers cung cấp khả năng tách câu dựa trên ý nghĩa\n",
    "-   NLTK và spaCy triển khai tách dựa trên quy tắc ngôn ngữ\n",
    "-   KoNLPy chuyên về phân tích hình thái và tách văn bản tiếng Hàn.\n",
    "\n",
    "Thông qua hướng dẫn này, bạn sẽ hiểu các đặc điểm của từng công cụ và học cách chọn phương pháp tách văn bản phù hợp nhất cho dự án của mình.\n",
    "\n",
    "```bash\n",
    "pip install langchain_text_splitters tiktoken spacy sentence-transformers nltk konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of tiktoken\n",
    "\n",
    "`tiktoken` is a fast BPE tokenizer created by OpenAI.\n",
    "\n",
    "- Open the file ./data/appendix-keywords.txt and read its contents.\n",
    "- Store the read content in the file variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words\n"
     ]
    }
   ],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `CharacterTextSplitter` to split the text.\n",
    "\n",
    "- Initialize the text splitter using the `from_tiktoken_encoder` method, which is based on the Tiktoken encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # Set the chunk size to 300.\n",
    "    chunk_size=300,\n",
    "    # Ensure there is no overlap between chunks.\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))  # Output the number of divided chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This allows computers to better understand and process the text.\n",
      "Example: The word “apple” might be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\n",
      "Example: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "Tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Print the first element of the texts list.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tham khảo**\n",
    "\n",
    "-   Khi sử dụng `CharacterTextSplitter.from_tiktoken_encoder`, văn bản chỉ được tách bởi `CharacterTextSplitter`, và bộ mã hóa `Tiktoken` chỉ được sử dụng để đo và hợp nhất văn bản đã chia. (Điều này có nghĩa là văn bản đã tách có thể vượt quá kích thước chunk khi được đo bằng bộ mã hóa `Tiktoken`.)\n",
    "-   Khi sử dụng `RecursiveCharacterTextSplitter.from_tiktoken_encoder`, văn bản đã chia được đảm bảo không vượt quá kích thước chunk cho phép của mô hình ngôn ngữ. Nếu một văn bản đã tách vượt quá kích thước này, nó sẽ được chia đệ quy. Ngoài ra, bạn có thể tải trực tiếp bộ tách `Tiktoken`, đảm bảo rằng mỗi phần tách nhỏ hơn kích thước chunk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of TokenTextSplitter\n",
    "\n",
    "Use the `TokenTextSplitter` class to split the text into token-based chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This allows computers to better understand and process the text.\n",
      "Example: The word “apple” might be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token refers to a smaller unit of text obtained\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 10.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 0.\n",
    ")\n",
    "\n",
    "# Split the state_of_the_union text into chunks.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first chunk of the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of spaCy\n",
    "\n",
    "spaCy là một thư viện phần mềm mã nguồn mở cho xử lý ngôn ngữ tự nhiên nâng cao, được viết bằng ngôn ngữ lập trình Python và Cython.\n",
    "\n",
    "Một lựa chọn thay thế khác cho NLTK là sử dụng bộ mã hóa spaCy.\n",
    "\n",
    "1.  Cách văn bản được chia: Văn bản được chia bằng cách sử dụng bộ mã hóa spaCy.\n",
    "2.  Cách kích thước chunk được đo: Nó được đo bằng số lượng ký tự."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: \n"
     ]
    }
   ],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a text splitter using the `SpacyTextSplitter` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# Ignore  warning messages.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create the SpacyTextSplitter.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 200.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `split_text` method of the `text_splitter` object to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n",
      "Created a chunk of size 315, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 263, which is longer than the specified 200\n",
      "Created a chunk of size 289, which is longer than the specified 200\n",
      "Created a chunk of size 212, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n",
      "Created a chunk of size 207, which is longer than the specified 200\n",
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of SentenceTransformers\n",
    "\n",
    "`SentenceTransformersTokenTextSplitter` là một bộ tách văn bản chuyên dụng cho các mô hình `sentence-transformer`.\n",
    "\n",
    "Hành vi mặc định của nó là chia văn bản thành các chunk phù hợp với cửa sổ token của mô hình sentence-transformer đang được sử dụng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3022d9b3484d7fbdbb8954c99d1776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42e748627d44320a2dfd7e37411411f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc1fbff51b543628d065af9253916a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7040dfecc64529b9313d214422b921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9705fcc6b5da4159a6d9b09852e76697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefd66ded55749b0b55d1f72d9379ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce141bf475049aaa20cb835b6868ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19667d4ab8f54774afdac0fb14e7bcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb674cebe0e847dd9203f69ac59ba2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcf339a3c8f4b09ba5ef3ba4e755aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02502ab060384e68948c5fea6d14ac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create a sentence splitter and set the overlap between chunks to 50.\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: \n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code counts the number of tokens in the text stored in `the file` variable, excluding the count of start and stop tokens, and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2121\n"
     ]
    }
   ],
   "source": [
    "count_start_and_stop_tokens = 2  # Set the number of start and stop tokens to 2.\n",
    "\n",
    "# Subtract the count of start and stop tokens from the total number of tokens in the text.\n",
    "text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # Print the calculated number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `splitter.split_text()` function to split the text stored in the `text_to_split` variable into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=file)  # Split the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used for tasks like retrieval, classification, and other data analysis. example : word embedding vectors can be stored in a database for quick access. related keywords : embedding, database, vectorization sql definition : sql ( structured query language ) is a programming language for managing data in databases. it supports operations like querying, modifying, inserting, and deleting data. example : select * from users where age > 18 ; retrieves information about users older than 18. related keywords : database, query, data management csv definition : csv ( comma - separated values ) is a file format for storing data where each value is separated by a comma. it is often used for simple data storage and exchange in tabular form. example : a csv file with headers “ name, age, job ” might contain data like “ john doe, 30, developer ”. related keywords : file format, data handling, data exchange json definition : json ( javascript object notation ) is a lightweight data exchange format that represents data objects in a human - and machine - readable text format. example : { \" name \" : \" john doe \", \" age \" : 30, \" job \" : \" developer \" } is an example of json data. related keywords : data exchange, web development, api transformer definition : a transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. it is based on the attention mechanism. example : google translate uses transformer models to perform translations between languages. related keywords : deep learning, natural language processing, attention huggingface definition : huggingface is a library that provides pre - trained models and tools for natural language processing, making nlp tasks more accessible to researchers and developers. example : huggingface ’ s transformers library can be used\n"
     ]
    }
   ],
   "source": [
    "# Print the 0th chunk.\n",
    "print(text_chunks[1])  # Print the second chunk from the divided text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of NLTK\n",
    "\n",
    "Natural Language Toolkit (NLTK) là một thư viện và tập hợp các chương trình cho xử lý ngôn ngữ tự nhiên (NLP) tiếng Anh, được viết bằng ngôn ngữ lập trình Python.\n",
    "\n",
    "Thay vì chỉ đơn giản là tách theo \"\\n\\n\", NLTK có thể được sử dụng để tách văn bản dựa trên bộ mã hóa NLTK.\n",
    "\n",
    "1.  Phương pháp tách văn bản: Văn bản được tách bằng cách sử dụng bộ mã hóa NLTK.\n",
    "2.  Đo kích thước chunk: Kích thước được đo bằng số lượng ký tự.\n",
    "3.  `nltk` (Natural Language Toolkit) là một thư viện Python cho xử lý ngôn ngữ tự nhiên.\n",
    "4.  Nó hỗ trợ các tác vụ NLP khác nhau như tiền xử lý văn bản, tokenization, phân tích hình thái và gắn thẻ part-of-speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/dino/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: \n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a text splitter using the `NLTKTextSplitter` class.\n",
    "- Set the `chunk_size` parameter to 1000 to split the text into chunks of up to 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 200.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 288, which is longer than the specified 200\n",
      "Created a chunk of size 245, which is longer than the specified 200\n",
      "Created a chunk of size 269, which is longer than the specified 200\n",
      "Created a chunk of size 279, which is longer than the specified 200\n",
      "Created a chunk of size 212, which is longer than the specified 200\n",
      "Created a chunk of size 205, which is longer than the specified 200\n",
      "Created a chunk of size 281, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of Hugging Face tokenizer\n",
    "\n",
    "Hugging Face cung cấp nhiều bộ mã hóa (tokenizers) khác nhau.\n",
    "\n",
    "Đoạn mã này minh họa cách tính độ dài token của một văn bản bằng một trong các bộ mã hóa của Hugging Face, GPT2TokenizerFast.\n",
    "\n",
    "Phương pháp tách văn bản như sau:\n",
    "\n",
    "-   Văn bản được tách ở cấp độ ký tự.\n",
    "\n",
    "Việc đo kích thước chunk được xác định như sau:\n",
    "\n",
    "-   Nó dựa trên số lượng token được tính bởi bộ mã hóa Hugging Face.\n",
    "-   Một đối tượng `tokenizer` được tạo bằng lớp `GPT2TokenizerFast`.\n",
    "-   Phương thức `from_pretrained` được gọi để tải mô hình bộ mã hóa \"gpt2\" được huấn luyện trước.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load the GPT-2 tokenizer.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: \n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    # Use the Hugging Face tokenizer to create a CharacterTextSplitter object.\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\n",
      "Example: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector form. It is used for tasks like retrieval, classification, and other data analysis.\n",
      "Example: Word embedding vectors can be stored in a database for quick access.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "\n",
      "SQL\n",
      "\n",
      "Definition: SQL (Structured Query Language) is a programming language for managing data in databases. It supports operations like querying, modifying, inserting, and deleting data.\n",
      "Example: SELECT * FROM users WHERE age > 18; retrieves information about users older than 18.\n",
      "Related Keywords: Database, Query, Data Management\n",
      "\n",
      "CSV\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])  # Print the first element of the texts list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
