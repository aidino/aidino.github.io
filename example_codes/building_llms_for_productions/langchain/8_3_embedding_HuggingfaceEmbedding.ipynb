{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Embeddings\n",
    "\n",
    "- `Hugging Face` cung c·∫•p m·ªôt lo·∫°t c√°c **m√¥ h√¨nh embedding** mi·ªÖn ph√≠, cho ph√©p th·ª±c hi·ªán nhi·ªÅu t√°c v·ª• `embedding` kh√°c nhau m·ªôt c√°ch d·ªÖ d√†ng.\n",
    "- Trong h∆∞·ªõng d·∫´n n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng `langchain_huggingface` ƒë·ªÉ x√¢y d·ª±ng m·ªôt **h·ªá th·ªëng t√¨m ki·∫øm ƒë∆°n gi·∫£n d·ª±a tr√™n embedding vƒÉn b·∫£n**.\n",
    "- C√°c m√¥ h√¨nh sau s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng cho **Text Embedding**:\n",
    "\n",
    "    - 1Ô∏è‚É£ **multilingual-e5-large-instruct**: M·ªôt m√¥ h√¨nh `embedding` ƒëa ng√¥n ng·ªØ d·ª±a tr√™n h∆∞·ªõng d·∫´n.\n",
    "    - 2Ô∏è‚É£ **multilingual-e5-large**: M·ªôt m√¥ h√¨nh `embedding` ƒëa ng√¥n ng·ªØ m·∫°nh m·∫Ω.\n",
    "    - 3Ô∏è‚É£ **bge-m3**: ƒê∆∞·ª£c t·ªëi ∆∞u h√≥a cho x·ª≠ l√Ω vƒÉn b·∫£n quy m√¥ l·ªõn.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/aidino/LangChain-OpenTutorial/a589b7f082f5b0a358921d3e6e9a0c8d97978eb8/08-Embedding/assets/03-huggingfaceembeddings-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è **C√°c c·∫•u h√¨nh sau s·∫Ω ƒë∆∞·ª£c thi·∫øt l·∫≠p**\n",
    "\n",
    "-   **C√†i ƒë·∫∑t ƒë·∫ßu ra Jupyter Notebook**\n",
    "    -   Hi·ªÉn th·ªã th√¥ng b√°o l·ªói chu·∫©n (`stderr`) tr·ª±c ti·∫øp thay v√¨ ghi l·∫°i ch√∫ng.\n",
    "-   **C√†i ƒë·∫∑t c√°c g√≥i c·∫ßn thi·∫øt**\n",
    "    -   ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c ph·ª• thu·ªôc c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.\n",
    "-   **Thi·∫øt l·∫≠p kh√≥a API**\n",
    "    -   ƒê·ªãnh c·∫•u h√¨nh kh√≥a API ƒë·ªÉ x√°c th·ª±c.\n",
    "-   **Thi·∫øt l·∫≠p l·ª±a ch·ªçn thi·∫øt b·ªã PyTorch**\n",
    "    -   T·ª± ƒë·ªông ch·ªçn thi·∫øt b·ªã t√≠nh to√°n t·ªëi ∆∞u (CPU, CUDA ho·∫∑c MPS).\n",
    "        -   `{\"device\": \"mps\"}`: Th·ª±c hi·ªán t√≠nh to√°n `embedding` b·∫±ng **MPS** thay v√¨ GPU. (D√†nh cho ng∆∞·ªùi d√πng Mac)\n",
    "        -   `{\"device\": \"cuda\"}`: Th·ª±c hi·ªán t√≠nh to√°n `embedding` b·∫±ng **GPU**. (D√†nh cho ng∆∞·ªùi d√πng Linux v√† Windows, y√™u c·∫ßu c√†i ƒë·∫∑t CUDA)\n",
    "        -   `{\"device\": \"cpu\"}`: Th·ª±c hi·ªán t√≠nh to√°n `embedding` b·∫±ng **CPU**. (Kh·∫£ d·ª•ng cho t·∫•t c·∫£ ng∆∞·ªùi d√πng)\n",
    "-   **ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ c·ª•c b·ªô m√¥ h√¨nh Embedding**\n",
    "    -   X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô ƒë·ªÉ l∆∞u tr·ªØ c√°c m√¥ h√¨nh `embedding`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using CPU\n",
      "üñ•Ô∏è Current device in use: cpu\n"
     ]
    }
   ],
   "source": [
    "# Automatically select the appropriate device\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if platform.system() == \"Darwin\":  # macOS specific\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
    "            return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"‚úÖ Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = get_device()\n",
    "print(\"üñ•Ô∏è Current device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Local Storage Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the download path to ./cache/\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Embedding-Based Search Tutorial\n",
    "\n",
    "To perform **embedding-based search,** we prepare both a **Query** and **Documents.**  \n",
    "\n",
    "1. Query  \n",
    "- Write a **key question** that will serve as the basis for the search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "q = \"Please tell me more about LangChain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Documents  \n",
    "- Prepare **multiple documents (texts)** that will serve as the target for the search.  \n",
    "- Each document will be **embedded** to enable semantic search capabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents for Text Embedding\n",
    "docs = [\n",
    "    \"Hi, nice to meet you.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models.\",\n",
    "    \"The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\",\n",
    "    \"LangChain simplifies the process of building applications with large-scale language models.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Text Embedding Model Should You Use?\n",
    "- Leverage the **MTEB leaderboard** and **free embedding models** to confidently select and utilize the **best-performing text embedding models** for your projects! üöÄ  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **What is MTEB (Massive Text Embedding Benchmark)?**  \n",
    "- **MTEB** is a benchmark designed to **systematically and objectively evaluate** the performance of text embedding models.  \n",
    "    - **Purpose:** To **fairly compare** the performance of embedding models.  \n",
    "    - **Evaluation Tasks:** Includes tasks like **Classification,**  **Retrieval,**  **Clustering,**  and **Semantic Similarity.**  \n",
    "    - **Supported Models:** A wide range of **text embedding models available on Hugging Face.**  \n",
    "    - **Results:** Displayed as **scores,**  with top-performing models ranked on the **leaderboard.**  \n",
    "\n",
    "üîó [ **MTEB Leaderboard (Hugging Face)** ](https://huggingface.co/spaces/mteb/leaderboard)  \n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è **Models Used in This Tutorial**  \n",
    "\n",
    "| **Embedding Model** | **Description** |\n",
    "|----------|----------|\n",
    "| 1Ô∏è‚É£ **multilingual-e5-large-instruct** | Offers strong multilingual support with consistent results. |\n",
    "| 2Ô∏è‚É£ **multilingual-e5-large** | A powerful multilingual embedding model. |\n",
    "| 3Ô∏è‚É£ **bge-m3** | Optimized for large-scale text processing, excelling in retrieval and semantic similarity tasks. |\n",
    "\n",
    "1Ô∏è‚É£ **multilingual-e5-large-instruct**\n",
    "![](./assets/03-huggingfaceembeddings-leaderboard-01.png)\n",
    "\n",
    "2Ô∏è‚É£ **multilingual-e5-large**\n",
    "![](./assets/03-huggingfaceembeddings-leaderboard-02.png)\n",
    "\n",
    "3Ô∏è‚É£ **bge-m3**\n",
    "![](./assets/03-huggingfaceembeddings-leaderboard-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng - Similarity Calculation\n",
    "\n",
    "**T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng t√≠ch v√¥ h∆∞·ªõng vector**\n",
    "\n",
    "-   ƒê·ªô t∆∞∆°ng ƒë·ªìng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng **t√≠ch v√¥ h∆∞·ªõng** c·ªßa c√°c vector.\n",
    "\n",
    "-   **C√¥ng th·ª©c t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng:**\n",
    "\n",
    "$$ \\text{similarities} = \\mathbf{query} \\cdot \\mathbf{documents}^T $$\n",
    "\n",
    "---\n",
    "\n",
    "### üìê **√ù nghƒ©a to√°n h·ªçc c·ªßa t√≠ch v√¥ h∆∞·ªõng vector**\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a t√≠ch v√¥ h∆∞·ªõng vector**\n",
    "\n",
    "**T√≠ch v√¥ h∆∞·ªõng** c·ªßa hai vector, $\\mathbf{a}$ v√† $\\mathbf{b}$, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a to√°n h·ªçc nh∆∞ sau:\n",
    "\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "---\n",
    "\n",
    "**M·ªëi quan h·ªá v·ªõi ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine**\n",
    "\n",
    "**T√≠ch v√¥ h∆∞·ªõng** c≈©ng li√™n quan ƒë·∫øn **ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine** v√† tu√¢n theo t√≠nh ch·∫•t sau:\n",
    "\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta $$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "\n",
    "-   $\\|\\mathbf{a}\\|$ v√† $\\|\\mathbf{b}\\|$ ƒë·∫°i di·ªán cho **ƒë·ªô l·ªõn** (**chu·∫©n**, c·ª• th·ªÉ l√† chu·∫©n Euclidean) c·ªßa c√°c vector $\\mathbf{a}$ v√† $\\mathbf{b}$.\n",
    "-   $\\theta$ l√† **g√≥c gi·ªØa hai vector**.\n",
    "-   $\\cos \\theta$ ƒë·∫°i di·ªán cho **ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine** gi·ªØa hai vector.\n",
    "\n",
    "---\n",
    "\n",
    "**üîç Gi·∫£i th√≠ch t√≠ch v√¥ h∆∞·ªõng vector trong ƒë·ªô t∆∞∆°ng ƒë·ªìng**\n",
    "\n",
    "Khi **gi√° tr·ªã t√≠ch v√¥ h∆∞·ªõng l·ªõn** (gi√° tr·ªã d∆∞∆°ng l·ªõn):\n",
    "\n",
    "-   **ƒê·ªô l·ªõn** ($\\|\\mathbf{a}\\|$ v√† $\\|\\mathbf{b}\\|$) c·ªßa hai vector l·ªõn.\n",
    "-   **G√≥c** ($\\theta$) gi·ªØa hai vector nh·ªè (**$\\cos \\theta$ ti·∫øn g·∫ßn 1**).\n",
    "\n",
    "ƒêi·ªÅu n√†y cho th·∫•y hai vector ch·ªâ theo **h∆∞·ªõng t∆∞∆°ng t·ª±** v√† **t∆∞∆°ng ƒë·ªìng v·ªÅ ng·ªØ nghƒ©a h∆°n**, ƒë·∫∑c bi·ªát khi ƒë·ªô l·ªõn c·ªßa ch√∫ng c≈©ng l·ªõn.\n",
    "\n",
    "---\n",
    "\n",
    "### üìè **T√≠nh to√°n ƒë·ªô l·ªõn vector (chu·∫©n)**\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a chu·∫©n Euclidean**\n",
    "\n",
    "ƒê·ªëi v·ªõi vector $\\mathbf{a} = [a_1, a_2, \\ldots, a_n]$, **chu·∫©n Euclidean** $\\|\\mathbf{a}\\|$ ƒë∆∞·ª£c t√≠nh nh∆∞ sau:\n",
    "\n",
    "$$ \\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2} $$\n",
    "\n",
    "**ƒê·ªô l·ªõn** n√†y ƒë·∫°i di·ªán cho **chi·ªÅu d√†i** ho·∫∑c **k√≠ch th∆∞·ªõc** c·ªßa vector trong kh√¥ng gian ƒëa chi·ªÅu.\n",
    "\n",
    "---\n",
    "\n",
    "Hi·ªÉu ƒë∆∞·ª£c nh·ªØng n·ªÅn t·∫£ng to√°n h·ªçc n√†y gi√∫p ƒë·∫£m b·∫£o t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng ch√≠nh x√°c, cho ph√©p hi·ªáu su·∫•t t·ªët h∆°n trong c√°c t√°c v·ª• nh∆∞ **t√¨m ki·∫øm ng·ªØ nghƒ©a**, **h·ªá th·ªëng truy xu·∫•t** v√† **c√¥ng c·ª• ƒë·ªÅ xu·∫•t**. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Similarity calculation between `embedded_query` and `embedded_document` \n",
    "- `embed_documents` : For embedding multiple texts (documents)\n",
    "- `embed_query` : For embedding a single text (query)\n",
    "\n",
    "We've implemented a method to search for the most relevant documents using **text embeddings.** \n",
    "- Let's use `search_similar_documents(q, docs, hf_embeddings)` to find the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def search_similar_documents(q, docs, hf_embeddings):\n",
    "    \"\"\"\n",
    "    Search for the most relevant documents based on a query using text embeddings.\n",
    "\n",
    "    Args:\n",
    "        q (str): The query string for which relevant documents are to be found.\n",
    "        docs (list of str): A list of document strings to compare against the query.\n",
    "        hf_embeddings: An embedding model object with `embed_query` and `embed_documents` methods.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - embedded_query (numpy.ndarray): The embedding vector of the query.\n",
    "            - embedded_documents (numpy.ndarray): The embedding matrix of the documents.\n",
    "\n",
    "    Workflow:\n",
    "        1. Embed the query string into a numerical vector using `embed_query`.\n",
    "        2. Embed each document into numerical vectors using `embed_documents`.\n",
    "        3. Calculate similarity scores between the query and documents using the dot product.\n",
    "        4. Sort the documents based on their similarity scores in descending order.\n",
    "        5. Print the query and display the sorted documents by their relevance.\n",
    "        6. Return the query and document embeddings for further analysis if needed.\n",
    "    \"\"\"\n",
    "    # Embed the query and documents using the embedding model\n",
    "    embedded_query = hf_embeddings.embed_query(q)\n",
    "    embedded_documents = hf_embeddings.embed_documents(docs)\n",
    "\n",
    "    # Calculate similarity scores using dot product\n",
    "    similarity_scores = np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "\n",
    "    # Sort documents by similarity scores in descending order\n",
    "    sorted_idx = similarity_scores.argsort()[::-1]\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"[Query] {q}\\n\" + \"=\" * 40)\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        print(f\"[{i}] {docs[idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Return embeddings for potential further processing or analysis\n",
    "    return embedded_query, embedded_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·ªïng quan v·ªÅ HuggingFaceEndpointEmbeddings\n",
    "\n",
    "**HuggingFaceEndpointEmbeddings** l√† m·ªôt t√≠nh nƒÉng trong th∆∞ vi·ªán **LangChain** t·∫≠n d·ª•ng **ƒëi·ªÉm cu·ªëi Hugging Face Inference API** ƒë·ªÉ t·∫°o ra c√°c `embeddings` vƒÉn b·∫£n m·ªôt c√°ch li·ªÅn m·∫°ch.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **C√°c kh√°i ni·ªám ch√≠nh**\n",
    "\n",
    "1.  **Hugging Face Inference API**\n",
    "    -   Truy c·∫≠p c√°c m√¥ h√¨nh `embedding` ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc th√¥ng qua API c·ªßa Hugging Face.\n",
    "    -   Kh√¥ng c·∫ßn t·∫£i xu·ªëng m√¥ h√¨nh c·ª•c b·ªô; `embeddings` ƒë∆∞·ª£c t·∫°o tr·ª±c ti·∫øp th√¥ng qua API.\n",
    "\n",
    "2.  **T√≠ch h·ª£p LangChain**\n",
    "    -   D·ªÖ d√†ng t√≠ch h·ª£p k·∫øt qu·∫£ `embedding` v√†o quy tr√¨nh l√†m vi·ªác LangChain b·∫±ng giao di·ªán ti√™u chu·∫©n h√≥a c·ªßa n√≥.\n",
    "\n",
    "3.  **Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng**\n",
    "    -   T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng truy v·∫•n vƒÉn b·∫£n v√† t√†i li·ªáu\n",
    "    -   H·ªá th·ªëng t√¨m ki·∫øm v√† ƒë·ªÅ xu·∫•t\n",
    "    -   C√°c ·ª©ng d·ª•ng Hi·ªÉu ng√¥n ng·ªØ t·ª± nhi√™n (NLU)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **C√°c tham s·ªë ch√≠nh**\n",
    "\n",
    "-   `model`: ID m√¥ h√¨nh Hugging Face (v√≠ d·ª•: `BAAI/bge-m3`)\n",
    "-   `task`: T√°c v·ª• c·∫ßn th·ª±c hi·ªán (th∆∞·ªùng l√† `\"feature-extraction\"`)\n",
    "-   `api_key`: M√£ th√¥ng b√°o API Hugging Face c·ªßa b·∫°n\n",
    "-   `model_kwargs`: C√°c tham s·ªë c·∫•u h√¨nh m√¥ h√¨nh b·ªï sung\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **∆Øu ƒëi·ªÉm**\n",
    "\n",
    "-   **Kh√¥ng c·∫ßn t·∫£i xu·ªëng m√¥ h√¨nh c·ª•c b·ªô:** Truy c·∫≠p t·ª©c th√¨ qua API.\n",
    "-   **Kh·∫£ nƒÉng m·ªü r·ªông:** H·ªó tr·ª£ nhi·ªÅu m√¥ h√¨nh Hugging Face ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc.\n",
    "-   **T√≠ch h·ª£p li·ªÅn m·∫°ch:** D·ªÖ d√†ng t√≠ch h·ª£p `embeddings` v√†o quy tr√¨nh l√†m vi·ªác LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **L∆∞u √Ω**\n",
    "\n",
    "-   **H·ªó tr·ª£ API:** Kh√¥ng ph·∫£i t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë·ªÅu h·ªó tr·ª£ suy lu·∫≠n API.\n",
    "-   **T·ªëc ƒë·ªô & Chi ph√≠:** API mi·ªÖn ph√≠ c√≥ th·ªÉ c√≥ th·ªùi gian ph·∫£n h·ªìi ch·∫≠m h∆°n v√† gi·ªõi h·∫°n s·ª≠ d·ª•ng.\n",
    "\n",
    "---\n",
    "\n",
    "V·ªõi **HuggingFaceEndpointEmbeddings**, b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng t√≠ch h·ª£p c√°c m√¥ h√¨nh `embedding` m·∫°nh m·∫Ω c·ªßa Hugging Face v√†o **quy tr√¨nh l√†m vi·ªác LangChain** c·ªßa m√¨nh ƒë·ªÉ c√≥ c√°c gi·∫£i ph√°p NLP hi·ªáu qu·∫£ v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let‚Äôs use the `intfloat/multilingual-e5-large-instruct` model via the API to search for the most relevant documents using text embeddings.\n",
    "\n",
    "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_endpoint_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the most relevant documents based on a query using text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.07 ms, sys: 1.03 ms, total: 9.1 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the query and documents using the embedding model\n",
    "embedded_query = hf_endpoint_embeddings.embed_query(q)\n",
    "embedded_documents = hf_endpoint_embeddings.embed_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity scores using dot product\n",
    "similarity_scores = np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "\n",
    "# Sort documents by similarity scores in descending order\n",
    "sorted_idx = similarity_scores.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Please tell me more about LangChain.\n",
      "========================================\n",
      "[0] LangChain simplifies the process of building applications with large language models.\n",
      "\n",
      "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
      "\n",
      "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
      "\n",
      "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n",
      "[4] Hi, nice to meet you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(f\"[Query] {q}\\n\" + \"=\" * 40)\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {docs[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingFace Endpoint Embedding]\n",
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Document Dimension: \t1024\n",
      "Query Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(\"[HuggingFace Endpoint Embedding]\")\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
    "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch√∫ng ta c√≥ th·ªÉ x√°c minh r·∫±ng k√≠ch th∆∞·ªõc c·ªßa `embedded_documents` v√† `embedded_query` nh·∫•t qu√°n.\n",
    "\n",
    "B·∫°n c≈©ng c√≥ th·ªÉ th·ª±c hi·ªán t√¨m ki·∫øm b·∫±ng ph∆∞∆°ng th·ª©c `search_similar_documents` m√† ch√∫ng ta ƒë√£ tri·ªÉn khai tr∆∞·ªõc ƒë√≥.\n",
    "T·ª´ b√¢y gi·ªù, h√£y s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c n√†y cho c√°c t√¨m ki·∫øm c·ªßa ch√∫ng ta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Please tell me more about LangChain.\n",
      "========================================\n",
      "[0] LangChain simplifies the process of building applications with large language models.\n",
      "\n",
      "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
      "\n",
      "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
      "\n",
      "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n",
      "[4] Hi, nice to meet you.\n",
      "\n",
      "CPU times: user 9.11 ms, sys: 54 Œºs, total: 9.16 ms\n",
      "Wall time: 709 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_endpoint_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFaceEmbeddings Overview\n",
    "\n",
    "- **HuggingFaceEmbeddings** l√† m·ªôt t√≠nh nƒÉng trong th∆∞ vi·ªán **LangChain** cho ph√©p chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n th√†nh vect∆° b·∫±ng c√°ch s·ª≠ d·ª•ng **Hugging Face embedding models.**\n",
    "- L·ªõp n√†y t·∫£i xu·ªëng v√† v·∫≠n h√†nh c√°c m√¥ h√¨nh Hugging Face **locally** ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **C√°c kh√°i ni·ªám ch√≠nh**\n",
    "\n",
    "1. **Hugging Face Pre-trained Models**\n",
    "   - S·ª≠ d·ª•ng c√°c m√¥ h√¨nh embedding ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc (pre-trained) do Hugging Face cung c·∫•p.\n",
    "   - T·∫£i xu·ªëng c√°c m√¥ h√¨nh **locally** ƒë·ªÉ th·ª±c hi·ªán tr·ª±c ti·∫øp c√°c ho·∫°t ƒë·ªông embedding.\n",
    "\n",
    "2. **LangChain Integration**\n",
    "   - T√≠ch h·ª£p li·ªÅn m·∫°ch v·ªõi quy tr√¨nh l√†m vi·ªác c·ªßa LangChain b·∫±ng giao di·ªán ti√™u chu·∫©n h√≥a c·ªßa n√≥.\n",
    "\n",
    "3. **Use Cases**\n",
    "   - T√≠nh to√°n ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa truy v·∫•n vƒÉn b·∫£n v√† t√†i li·ªáu\n",
    "   - H·ªá th·ªëng t√¨m ki·∫øm v√† ƒë·ªÅ xu·∫•t\n",
    "   - ·ª®ng d·ª•ng Natural Language Understanding (NLU)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **C√°c tham s·ªë ch√≠nh**\n",
    "\n",
    "- `model_name`: ID m√¥ h√¨nh Hugging Face (v√≠ d·ª•: `sentence-transformers/all-MiniLM-L6-v2`)\n",
    "- `model_kwargs`: C√°c tham s·ªë c·∫•u h√¨nh m√¥ h√¨nh b·ªï sung (v√≠ d·ª•: c√†i ƒë·∫∑t thi·∫øt b·ªã GPU/CPU)\n",
    "- `encode_kwargs`: C√°c c√†i ƒë·∫∑t b·ªï sung cho vi·ªác t·∫°o embedding\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **∆Øu ƒëi·ªÉm**\n",
    "\n",
    "- **Local Embedding Operations:** Th·ª±c hi·ªán embedding c·ª•c b·ªô m√† kh√¥ng c·∫ßn k·∫øt n·ªëi internet.\n",
    "- **High Performance:** S·ª≠ d·ª•ng c√†i ƒë·∫∑t GPU ƒë·ªÉ t·∫°o embedding nhanh h∆°n.\n",
    "- **Model Variety:** H·ªó tr·ª£ nhi·ªÅu lo·∫°i m√¥ h√¨nh Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **L∆∞u √Ω**\n",
    "\n",
    "- **Local Storage Requirement:** C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ph·∫£i ƒë∆∞·ª£c t·∫£i xu·ªëng c·ª•c b·ªô.\n",
    "- **Environment Configuration:** Hi·ªáu su·∫•t c√≥ th·ªÉ thay ƒë·ªïi t√πy thu·ªôc v√†o c√†i ƒë·∫∑t thi·∫øt b·ªã GPU/CPU.\n",
    "\n",
    "---\n",
    "\n",
    "V·ªõi **HuggingFaceEmbeddings**, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng hi·ªáu qu·∫£ **Hugging Face's powerful embedding models** trong m√¥i tr∆∞·ªùng **local**, cho ph√©p c√°c gi·∫£i ph√°p NLP linh ho·∫°t v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's download the embedding model locally, perform embeddings, and search for the most relevant documents.\n",
    "\n",
    "`intfloat/multilingual-e5-large-instruct` \n",
    "\n",
    "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43502e30ad0a4517afcff0ffefa3b188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9275dc03316b4bec95010cb27882bc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fe30a7d2494b0fb41fefe0aa6f9053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7788099c27c642fd917cdfae0c10729e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_xlm-roberta_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c48314d7414e55b613f447119ecc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8f8fd7b2c1469890c74d926caef3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d415e610d3ed45f6957d865cc3c22abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f14ff0b5e524c65a73aeb413287d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602635d87c934a7fafd7b777726c40a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e401dc33d5c846a286b6c08a454aea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef1ce3e608b4ebab5871ad64a6dcf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Please tell me more about LangChain.\n",
      "========================================\n",
      "[0] LangChain simplifies the process of building applications with large language models.\n",
      "\n",
      "[1] LangChain simplifies the process of building applications with large-scale language models.\n",
      "\n",
      "[2] The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.\n",
      "\n",
      "[3] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n",
      "[4] Hi, nice to meet you.\n",
      "\n",
      "CPU times: user 10.6 s, sys: 0 ns, total: 10.6 s\n",
      "Wall time: 681 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedded_query, embedded_documents = search_similar_documents(q, docs, hf_embeddings_e5_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Document Dimension: \t1024\n",
      "Query Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Document Dimension: \\t{len(embedded_documents[0])}\")\n",
    "print(f\"Query Dimension: \\t{len(embedded_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlagEmbedding Usage Guide\n",
    "\n",
    "\n",
    "- **FlagEmbedding** l√† m·ªôt framework embedding ti√™n ti·∫øn ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **BAAI (Beijing Academy of Artificial Intelligence).**\n",
    "- N√≥ h·ªó tr·ª£ **various embedding approaches** v√† ch·ªß y·∫øu ƒë∆∞·ª£c s·ª≠ d·ª•ng v·ªõi m√¥ h√¨nh **BGE (BAAI General Embedding).**\n",
    "- FlagEmbedding v∆∞·ª£t tr·ªôi trong c√°c nhi·ªám v·ª• nh∆∞ **semantic search**, **natural language processing (NLP)** v√† **recommendation systems.**\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **C√°c kh√°i ni·ªám c·ªët l√µi c·ªßa FlagEmbedding**\n",
    "\n",
    "1Ô∏è‚É£ `Dense Embedding`\n",
    "- ƒê·ªãnh nghƒ©a: Bi·ªÉu di·ªÖn √Ω nghƒ©a t·ªïng th·ªÉ c·ªßa m·ªôt vƒÉn b·∫£n d∆∞·ªõi d·∫°ng m·ªôt vect∆° m·∫≠t ƒë·ªô cao duy nh·∫•t.\n",
    "- ∆Øu ƒëi·ªÉm: N·∫Øm b·∫Øt hi·ªáu qu·∫£ s·ª± t∆∞∆°ng ƒë·ªìng v·ªÅ ng·ªØ nghƒ©a.\n",
    "- Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng: T√¨m ki·∫øm ng·ªØ nghƒ©a, t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa t√†i li·ªáu.\n",
    "\n",
    "2Ô∏è‚É£ `Lexical Embedding`\n",
    "- ƒê·ªãnh nghƒ©a: Ph√¢n t√°ch vƒÉn b·∫£n th√†nh c√°c th√†nh ph·∫ßn c·∫•p t·ª´, nh·∫•n m·∫°nh vi·ªác kh·ªõp t·ª´.\n",
    "- ∆Øu ƒëi·ªÉm: ƒê·∫£m b·∫£o kh·ªõp ch√≠nh x√°c c√°c t·ª´ ho·∫∑c c·ª•m t·ª´ c·ª• th·ªÉ.\n",
    "- Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng: T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a, kh·ªõp t·ª´ ch√≠nh x√°c.\n",
    "\n",
    "3Ô∏è‚É£ `Multi-Vector Embedding`\n",
    "- ƒê·ªãnh nghƒ©a: Chia m·ªôt t√†i li·ªáu th√†nh nhi·ªÅu vect∆° ƒë·ªÉ bi·ªÉu di·ªÖn.\n",
    "- ∆Øu ƒëi·ªÉm: Cho ph√©p bi·ªÉu di·ªÖn chi ti·∫øt h∆°n ƒë·ªëi v·ªõi c√°c vƒÉn b·∫£n d√†i ho·∫∑c c√°c ch·ªß ƒë·ªÅ ƒëa d·∫°ng.\n",
    "- Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng: Ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu ph·ª©c t·∫°p, kh·ªõp ch·ªß ƒë·ªÅ chi ti·∫øt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "FlagEmbedding cung c·∫•p m·ªôt **flexible and powerful toolkit** ƒë·ªÉ t·∫≠n d·ª•ng embeddings tr√™n m·ªôt lo·∫°t c√°c **NLP tasks v√† semantic search applications.** üöÄ\n",
    "\n",
    "ƒêo·∫°n m√£ sau ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ki·ªÉm so√°t **tokenizer parallelism** trong th∆∞ vi·ªán `transformers` c·ªßa Hugging Face:\n",
    "\n",
    "- `TOKENIZERS_PARALLELISM = \"true\"` ‚Üí **Optimized for speed**, ph√π h·ª£p cho x·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn.\n",
    "- `TOKENIZERS_PARALLELISM = \"false\"` ‚Üí **Ensures stability**, ngƒÉn ng·ª´a xung ƒë·ªôt v√† ƒëi·ªÅu ki·ªán ch·∫°y ƒëua (race conditions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install FlagEmbedding\n",
    "%pip install -qU FlagEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è **Tham s·ªë ch√≠nh**\n",
    "\n",
    "`BGEM3FlagModel`\n",
    "- `model_name`: **Model ID** c·ªßa Hugging Face (v√≠ d·ª•: `BAAI/bge-m3`).\n",
    "- `use_fp16`: Khi ƒë∆∞·ª£c ƒë·∫∑t th√†nh **True**, gi·∫£m **memory usage** v√† c·∫£i thi·ªán **encoding speed.**\n",
    "\n",
    "`bge_embeddings.encode`\n",
    "- `batch_size`: X√°c ƒë·ªãnh **number of documents** c·∫ßn x·ª≠ l√Ω c√πng m·ªôt l√∫c.\n",
    "- `max_length`: ƒê·∫∑t **maximum token length** cho vi·ªác encoding documents.\n",
    "    - TƒÉng l√™n cho c√°c t√†i li·ªáu d√†i h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o m√£ h√≥a to√†n b·ªô n·ªôi dung.\n",
    "    - Gi√° tr·ªã qu√° l·ªõn c√≥ th·ªÉ **degrade performance.**\n",
    "- `return_dense`: Khi ƒë∆∞·ª£c ƒë·∫∑t th√†nh **True**, ch·ªâ tr·∫£ v·ªÅ **Dense Vectors.**\n",
    "- `return_sparse`: Khi ƒë∆∞·ª£c ƒë·∫∑t th√†nh **True**, tr·∫£ v·ªÅ **Sparse Vectors.**\n",
    "- `return_colbert_vecs`: Khi ƒë∆∞·ª£c ƒë·∫∑t th√†nh **True**, tr·∫£ v·ªÅ **ColBERT-style vectors.**\n",
    "\n",
    "### 1Ô∏è‚É£ **V√≠ d·ª• v·ªÅ Dense Vector Embedding**\n",
    "- ƒê·ªãnh nghƒ©a: Bi·ªÉu di·ªÖn √Ω nghƒ©a t·ªïng th·ªÉ c·ªßa m·ªôt vƒÉn b·∫£n d∆∞·ªõi d·∫°ng m·ªôt vect∆° m·∫≠t ƒë·ªô cao duy nh·∫•t.\n",
    "- ∆Øu ƒëi·ªÉm: N·∫Øm b·∫Øt hi·ªáu qu·∫£ s·ª± t∆∞∆°ng ƒë·ªìng v·ªÅ ng·ªØ nghƒ©a.\n",
    "- Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng: T√¨m ki·∫øm ng·ªØ nghƒ©a, t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa t√†i li·ªáu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628ffea5bbd64f3db8938c7d5ce4dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429bc8bdc372438a81b762b486160a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6e952dbf1a4b4eb3d7ecb5d08af384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7989f305ac6748189684e85a767b7a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876255433a7742e6a80237b903fa9b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf74b76117a4eaa8fe3d78e6a2390df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "colbert_linear.pt:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d570ed39fe204bbf9ba056d45418fc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92f6274f7f44d399f24b09e15dc8a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f404332bf154336a29725600be66e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e92fcd23ab54ff48e63d0fa28a42289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bm25.jpg:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad0d6ff0a04191be2c0baa526baa2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0837e37e7d7c4556b58046050f362f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abd64d07e5042f2987d71deb819a55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd4735df00441faae2752cae75acbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "long.jpg:   0%|          | 0.00/485k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98615607eb24d848c473e1121a2b1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mkqa.jpg:   0%|          | 0.00/608k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e556af30d049e2a03d3d31d9bb015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nqa.jpg:   0%|          | 0.00/158k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0788fba1690b40888614747f021c533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "miracl.jpg:   0%|          | 0.00/576k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b940edb1d84ddeab057822385b6b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "long.jpg:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f4401274b64e049123efa1cf26c13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constant_7_attr__value:   0%|          | 0.00/65.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3292ee40c5c4c40af7d531c41dde37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "others.webp:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e5021b292a4d33b7eb456b5e65eae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/725k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3a51e4e4ea4c98a26b2f4ffa32b493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx_data:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7184396c8614222bda3ff6daf52bf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4185cd531c42e6a3695bb5263dd2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d039f089e941c6b5ce825daf049e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea420fcdba42d7a5acdd40645d7ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289b297242f24d75b900e2b8a37cdebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c59b03d205041e19b800db08b184598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sparse_linear.pt:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe4945542464d319132da4b764ed1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name,\n",
    "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
    ")\n",
    "\n",
    "# Encode documents with specified parameters\n",
    "embedded_documents_dense_vecs = bge_embeddings.encode(\n",
    "    sentences=docs,\n",
    "    batch_size=12,\n",
    "    max_length=8192,  # Reduce this value if your documents are shorter to speed up encoding.\n",
    ")[\"dense_vecs\"]\n",
    "\n",
    "# Query Encoding\n",
    "embedded_query_dense_vecs = bge_embeddings.encode(\n",
    "    sentences=[q],\n",
    "    batch_size=12,\n",
    "    max_length=8192,  # Reduce this value if your documents are shorter to speed up encoding.\n",
    ")[\"dense_vecs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_documents_dense_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query_dense_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please tell me more about LangChain.\n",
      "Most similar document: LangChain simplifies the process of building applications with large language models.\n"
     ]
    }
   ],
   "source": [
    "# Calculating Similarity Between Documents and Query\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(\n",
    "    embedded_query_dense_vecs, embedded_documents_dense_vecs\n",
    ")\n",
    "most_similar_idx = similarities.argmax()\n",
    "\n",
    "# Display the Most Similar Document\n",
    "print(f\"Question: {q}\")\n",
    "print(f\"Most similar document: {docs[most_similar_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce733da9c4ce428994cc80c9707529be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name,\n",
    "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
    ")\n",
    "\n",
    "# Encode documents with specified parameters\n",
    "embedded_documents_dense_vecs_default = bge_embeddings.encode(\n",
    "    sentences=docs, return_dense=True\n",
    ")[\"dense_vecs\"]\n",
    "\n",
    "# Query Encoding\n",
    "embedded_query_dense_vecs_default = bge_embeddings.encode(\n",
    "    sentences=[q], return_dense=True\n",
    ")[\"dense_vecs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please tell me more about LangChain.\n",
      "Most similar document: LangChain simplifies the process of building applications with large language models.\n"
     ]
    }
   ],
   "source": [
    "# Calculating Similarity Between Documents and Query\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(\n",
    "    embedded_query_dense_vecs_default, embedded_documents_dense_vecs_default\n",
    ")\n",
    "most_similar_idx = similarities.argmax()\n",
    "\n",
    "# Display the Most Similar Document\n",
    "print(f\"Question: {q}\")\n",
    "print(f\"Most similar document: {docs[most_similar_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ **Sparse(Lexical) Vector Embedding Example**\n",
    "\n",
    "**Sparse Embedding (Tr·ªçng s·ªë t·ª´ v·ª±ng)**\n",
    "- **Sparse embedding** l√† m·ªôt ph∆∞∆°ng ph√°p embedding s·ª≠ d·ª•ng **high-dimensional vectors where most values are zero.**\n",
    "- Ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng **lexical weight** t·∫°o ra embeddings b·∫±ng c√°ch xem x√©t **importance of each word.**\n",
    "\n",
    "**C√°ch th·ª©c ho·∫°t ƒë·ªông**\n",
    "1.  T√≠nh to√°n **lexical weight** cho m·ªói t·ª´. C√°c k·ªπ thu·∫≠t nh∆∞ **TF-IDF** ho·∫∑c **BM25** c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng.\n",
    "2.  ƒê·ªëi v·ªõi m·ªói t·ª´ trong m·ªôt t√†i li·ªáu ho·∫∑c truy v·∫•n, g√°n m·ªôt gi√° tr·ªã cho chi·ªÅu t∆∞∆°ng ·ª©ng c·ªßa **sparse vector** d·ª±a tr√™n lexical weight c·ªßa n√≥.\n",
    "3.  K·∫øt qu·∫£ l√†, c√°c t√†i li·ªáu v√† truy v·∫•n ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **high-dimensional vectors where most values are zero.**\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**\n",
    "- Ph·∫£n √°nh tr·ª±c ti·∫øp **importance of words.**\n",
    "- Cho ph√©p **precise matching** c·ªßa c√°c t·ª´ ho·∫∑c c·ª•m t·ª´ c·ª• th·ªÉ.\n",
    "- **Faster computation** so v·ªõi dense embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b50aafb7c974fa58e255a9a827e6227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name,\n",
    "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
    ")\n",
    "\n",
    "# Encode documents with specified parameters\n",
    "embedded_documents_sparse_vecs = bge_embeddings.encode(\n",
    "    sentences=docs, return_sparse=True\n",
    ")\n",
    "\n",
    "# Query Encoding\n",
    "embedded_query_sparse_vecs = bge_embeddings.encode(sentences=[q], return_sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_scores_0 = bge_embeddings.compute_lexical_matching_score(\n",
    "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
    "    embedded_documents_sparse_vecs[\"lexical_weights\"][0],\n",
    ")\n",
    "\n",
    "lexical_scores_1 = bge_embeddings.compute_lexical_matching_score(\n",
    "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
    "    embedded_documents_sparse_vecs[\"lexical_weights\"][1],\n",
    ")\n",
    "\n",
    "lexical_scores_2 = bge_embeddings.compute_lexical_matching_score(\n",
    "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
    "    embedded_documents_sparse_vecs[\"lexical_weights\"][2],\n",
    ")\n",
    "\n",
    "lexical_scores_3 = bge_embeddings.compute_lexical_matching_score(\n",
    "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
    "    embedded_documents_sparse_vecs[\"lexical_weights\"][3],\n",
    ")\n",
    "\n",
    "lexical_scores_4 = bge_embeddings.compute_lexical_matching_score(\n",
    "    embedded_query_sparse_vecs[\"lexical_weights\"][0],\n",
    "    embedded_documents_sparse_vecs[\"lexical_weights\"][4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Please tell me more about LangChain.\n",
      "====================\n",
      "Hi, nice to meet you. : 0.011874185875058174\n",
      "LangChain simplifies the process of building applications with large language models. : 0.23139647534117103\n",
      "The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively. : 0.1879164595156908\n",
      "LangChain simplifies the process of building applications with large-scale language models. : 0.22665631817653775\n",
      "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses. : 0.002352734562009573\n"
     ]
    }
   ],
   "source": [
    "print(f\"question: {q}\")\n",
    "print(\"====================\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(doc, f\": {eval(f'lexical_scores_{i}')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ **Multi-Vector(ColBERT) Embedding Example**\n",
    "\n",
    "**ColBERT** (Contextualized Late Interaction over BERT) l√† m·ªôt ph∆∞∆°ng ph√°p hi·ªáu qu·∫£ cho **document retrieval.**\n",
    "- Ph∆∞∆°ng ph√°p n√†y s·ª≠ d·ª•ng **multi-vector strategy** ƒë·ªÉ bi·ªÉu di·ªÖn c·∫£ t√†i li·ªáu v√† truy v·∫•n b·∫±ng nhi·ªÅu vect∆°.\n",
    "\n",
    "**C√°ch th·ª©c ho·∫°t ƒë·ªông**\n",
    "1.  T·∫°o **separate vector** cho m·ªói **token in a document**, d·∫´n ƒë·∫øn nhi·ªÅu vect∆° cho m·ªói t√†i li·ªáu.\n",
    "2.  T∆∞∆°ng t·ª±, t·∫°o **separate vector** cho m·ªói **token in a query.**\n",
    "3.  Trong qu√° tr√¨nh retrieval, t√≠nh to√°n **similarity** gi·ªØa m·ªói vect∆° token truy v·∫•n v√† t·∫•t c·∫£ c√°c vect∆° token t√†i li·ªáu.\n",
    "4.  T·ªïng h·ª£p c√°c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng n√†y ƒë·ªÉ t·∫°o ra **final retrieval score.**\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**\n",
    "- Cho ph√©p **fine-grained token-level matching.**\n",
    "- N·∫Øm b·∫Øt **contextual embeddings** m·ªôt c√°ch hi·ªáu qu·∫£.\n",
    "- Ho·∫°t ƒë·ªông hi·ªáu qu·∫£ ngay c·∫£ v·ªõi **long documents.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbca95a48224c32a39e41186b8d74f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name,\n",
    "    use_fp16=True,  # Enabling fp16 improves encoding speed with minimal precision trade-off.\n",
    ")\n",
    "\n",
    "# Encode documents with specified parameters\n",
    "embedded_documents_colbert_vecs = bge_embeddings.encode(\n",
    "    sentences=docs, return_colbert_vecs=True\n",
    ")\n",
    "\n",
    "# Query Encoding\n",
    "embedded_query_colbert_vecs = bge_embeddings.encode(\n",
    "    sentences=[q], return_colbert_vecs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_scores_0 = bge_embeddings.colbert_score(\n",
    "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
    "    embedded_documents_colbert_vecs[\"colbert_vecs\"][0],\n",
    ")\n",
    "\n",
    "colbert_scores_1 = bge_embeddings.colbert_score(\n",
    "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
    "    embedded_documents_colbert_vecs[\"colbert_vecs\"][1],\n",
    ")\n",
    "\n",
    "colbert_scores_2 = bge_embeddings.colbert_score(\n",
    "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
    "    embedded_documents_colbert_vecs[\"colbert_vecs\"][2],\n",
    ")\n",
    "\n",
    "colbert_scores_3 = bge_embeddings.colbert_score(\n",
    "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
    "    embedded_documents_colbert_vecs[\"colbert_vecs\"][3],\n",
    ")\n",
    "\n",
    "colbert_scores_4 = bge_embeddings.colbert_score(\n",
    "    embedded_query_colbert_vecs[\"colbert_vecs\"][0],\n",
    "    embedded_documents_colbert_vecs[\"colbert_vecs\"][4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Please tell me more about LangChain.\n",
      "====================\n",
      "Hi, nice to meet you. : 0.5088493824005127\n",
      "LangChain simplifies the process of building applications with large language models. : 0.703724205493927\n",
      "The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively. : 0.6633750796318054\n",
      "LangChain simplifies the process of building applications with large-scale language models. : 0.7055995464324951\n",
      "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses. : 0.38072970509529114\n"
     ]
    }
   ],
   "source": [
    "print(f\"question: {q}\")\n",
    "print(\"====================\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(doc, f\": {eval(f'colbert_scores_{i}')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° **∆Øu ƒëi·ªÉm c·ªßa FlagEmbedding**\n",
    "\n",
    "-   **Diverse Embedding Options:** H·ªó tr·ª£ c√°c ph∆∞∆°ng ph√°p **Dense, Lexical v√† Multi-Vector.**\n",
    "-   **High-Performance Models:** S·ª≠ d·ª•ng c√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc m·∫°nh m·∫Ω nh∆∞ **BGE.**\n",
    "-   **Flexibility:** Ch·ªçn ph∆∞∆°ng ph√°p embedding t·ªëi ∆∞u d·ª±a tr√™n **use case** c·ªßa b·∫°n.\n",
    "-   **Scalability:** C√≥ kh·∫£ nƒÉng th·ª±c hi·ªán embeddings tr√™n **large-scale datasets.**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **C√¢n nh·∫Øc**\n",
    "\n",
    "-   **Model Size:** M·ªôt s·ªë m√¥ h√¨nh c√≥ th·ªÉ y√™u c·∫ßu **significant storage capacity.**\n",
    "-   **Resource Requirements:** **GPU usage is recommended** cho c√°c t√≠nh to√°n vect∆° quy m√¥ l·ªõn.\n",
    "-   **Configuration Needs:** Hi·ªáu su·∫•t t·ªëi ∆∞u c√≥ th·ªÉ y√™u c·∫ßu **parameter tuning.**\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **So s√°nh vect∆° FlagEmbedding**\n",
    "\n",
    "| **Embedding Type** | **Strengths** | **Use Cases** |\n",
    "|---|---|---|\n",
    "| **Dense Vector** | Nh·∫•n m·∫°nh s·ª± t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a | Semantic search, document matching |\n",
    "| **Lexical Vector** | Kh·ªõp t·ª´ ch√≠nh x√°c | Keyword search, exact matches |\n",
    "| **Multi-Vector** | N·∫Øm b·∫Øt √Ω nghƒ©a ph·ª©c t·∫°p | Long document analysis, topic classification |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
