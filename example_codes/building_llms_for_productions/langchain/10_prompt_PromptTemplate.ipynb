{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template\n",
    "\n",
    "\n",
    "Tutorial này bao gồm cách tạo và sử dụng các **prompt templates** bằng **LangChain**.\n",
    "\n",
    "**Prompt templates** rất cần thiết để tạo ra các **prompts** động và linh hoạt, đáp ứng nhiều trường hợp sử dụng khác nhau, chẳng hạn như lịch sử hội thoại, đầu ra có cấu trúc và truy vấn chuyên biệt.\n",
    "\n",
    "Trong tutorial này, chúng ta sẽ khám phá các phương pháp để tạo các đối tượng **PromptTemplate**, áp dụng **partial variables**, quản lý các **templates** thông qua tệp YAML và tận dụng các công cụ nâng cao như **ChatPromptTemplate** và **MessagePlaceholder** để tăng cường chức năng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. Using the `from_template()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is 1 + 2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "# prompt.invoke({\"a\": 1, \"b\": 2})\n",
    "prompt.format(a=1, b=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Creating a `PromptTemplate` object and a prompt all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thành phố của Việt Nam là gì?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Thành phố của {country} là gì?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "prompt.format(country=\"Việt Nam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India và Viet Nam cái nào đẹp hơn?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"{country1} và {country2} cái nào đẹp hơn?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country1\"],\n",
    "    partial_variables={\n",
    "        \"country2\": \"Viet Nam\"\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt.format(country1=\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India và Korea cái nào đẹp hơn?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.partial_variables = {\"country2\": \"Korea\"}\n",
    "prompt.format(country1=\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `partial_variables`\n",
    "\n",
    "\n",
    "Sử dụng `partial_variables`, bạn có thể áp dụng hàm một phần. Điều này đặc biệt hữu ích khi có các `common variables` (biến chung) cần được chia sẻ.\n",
    "\n",
    "Các ví dụ phổ biến là `date` (ngày) hoặc `time` (thời gian).\n",
    "\n",
    "Giả sử bạn muốn chỉ định ngày hiện tại trong prompt của mình, việc hardcoding (mã hóa cứng) ngày vào prompt hoặc truyền nó cùng với các biến đầu vào khác có thể không thực tế. Trong trường hợp này, việc sử dụng một hàm trả về ngày hiện tại để sửa đổi prompt một phần sẽ thuận tiện hơn nhiều.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hôm nay là March 08, liệt kê 3 việc cần làm.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%B %d\")\n",
    "\n",
    "tenplate = \"Hôm nay là {date}, liệt kê {n} việc cần làm.\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=tenplate,\n",
    "    input_variables=['n'],\n",
    "    partial_variables={\n",
    "        \"date\": get_today\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt.format(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hôm nay là March 08, liệt kê 5 việc cần làm.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prompt Templates from `YAML` Files\n",
    "\n",
    "You can manage prompt templates in seperate `yaml` files and load using `load_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HIHI - HAHA - What is the color of Apple and Orange?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%B %d\")\n",
    "\n",
    "prompt = load_prompt(\"prompts/fruit_color.yaml\", encoding='utf-8')\n",
    "prompt.format(fruit1=\"Apple\", fruit2=\"Orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "\n",
    "`ChatPromptTemplate` có thể được sử dụng để bao gồm lịch sử hội thoại như một prompt.\n",
    "\n",
    "Các message được cấu trúc dưới dạng tuples theo định dạng `(role, message)` và được tạo thành một list.\n",
    "\n",
    "`role`\n",
    "\n",
    "* `system`: Một message thiết lập hệ thống, thường được sử dụng cho các prompt liên quan đến cài đặt toàn cục.\n",
    "* `human`: Một message nhập liệu của người dùng.\n",
    "* `ai`: Một message phản hồi của AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is the best thing in the Viet nam'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# From template\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the best thing in the {country}\")\n",
    "prompt.format(country=\"Viet nam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=' Bạn là một AI toàn năng ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='HAHA, đúng là như vậy', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Bạn có thể trả lời tôi một câu hỏi được không?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Vô tư điiii', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Ai đẹp trai nhất thế giới?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From messsages\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" Bạn là một AI toàn năng \"),\n",
    "        (\"ai\", \"HAHA, đúng là như vậy\"),\n",
    "        (\"human\", \"Bạn có thể trả lời tôi một câu hỏi được không?\"),\n",
    "        (\"ai\", \"Vô tư điiii\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.format_messages(question=\"Ai đẹp trai nhất thế giới?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MessagePlaceholder`\n",
    "\n",
    "LangChain cũng cung cấp `MessagePlaceholder`, cho phép kiểm soát hoàn toàn việc hiển thị các message trong quá trình định dạng.\n",
    "\n",
    "Điều này có thể hữu ích nếu bạn không chắc chắn nên sử dụng vai trò nào trong một message prompt template hoặc nếu bạn muốn chèn một list các message trong quá trình định dạng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong LangChain, `MessagePlaceHolder` là một thành phần được sử dụng để \"đặt chỗ\" (placeholder) cho một hoặc nhiều tin nhắn trong luồng hội thoại. Nó không chứa nội dung cụ thể mà chỉ giữ một vị trí trong danh sách các tin nhắn (messages), để sau này bạn có thể thay thế nó bằng dữ liệu thực tế, chẳng hạn như lịch sử trò chuyện, thông tin từ người dùng, hoặc phản hồi của AI.\n",
    "\n",
    "Nói đơn giản, nó giống như một \"hộp rỗng\" mà bạn đặt sẵn trong cấu trúc hội thoại, và khi cần, bạn sẽ \"đổ đầy\" nội dung vào đó.\n",
    "\n",
    "##### Khi nào dùng `MessagePlaceHolder`?\n",
    "- Khi bạn muốn linh hoạt trong việc thêm tin nhắn vào luồng hội thoại mà không cần xác định ngay nội dung cụ thể.\n",
    "- Khi bạn làm việc với các mô hình trò chuyện (chat models) và cần giữ chỗ cho lịch sử hội thoại hoặc thông tin động (dynamic data).\n",
    "\n",
    "##### Ví dụ dễ hiểu\n",
    "Hãy tưởng tượng bạn đang xây dựng một chatbot đặt pizza. Bạn muốn chatbot nhớ lịch sử trò chuyện của khách hàng (ví dụ: \"Tôi muốn pizza pepperoni\" hoặc \"Thêm phô mai đi\") để đưa ra phản hồi phù hợp. Nhưng lịch sử này thay đổi tùy theo mỗi khách hàng, nên bạn không thể cứng nhắc viết sẵn. Đây là lúc `MessagePlaceHolder` phát huy tác dụng.\n",
    "\n",
    "##### Code ví dụ\n",
    "Dưới đây là một ví dụ đơn giản bằng Python sử dụng LangChain:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Tạo một template hội thoại\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Bạn là trợ lý đặt pizza. Hãy giúp khách hàng một cách thân thiện!\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # Giữ chỗ cho lịch sử trò chuyện\n",
    "    (\"human\", \"Tôi muốn đặt một chiếc pizza.\")\n",
    "])\n",
    "\n",
    "# Giả lập lịch sử trò chuyện\n",
    "chat_history = [\n",
    "    (\"human\", \"Tôi thích pizza pepperoni.\"),\n",
    "    (\"ai\", \"Được thôi, bạn muốn kích thước bao nhiêu?\")\n",
    "]\n",
    "\n",
    "# Khởi tạo mô hình chat\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Tạo chuỗi (chain) để xử lý\n",
    "chain = prompt | model\n",
    "\n",
    "# Gọi chuỗi với lịch sử trò chuyện\n",
    "response = chain.invoke({\"chat_history\": chat_history})\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "##### Giải thích từng phần:\n",
    "1. **`ChatPromptTemplate.from_messages`**:\n",
    "   - Đây là nơi bạn định nghĩa cấu trúc hội thoại.\n",
    "   - Tin nhắn hệ thống (\"system\") cố định: \"Bạn là trợ lý đặt pizza...\"\n",
    "   - `MessagesPlaceholder(\"chat_history\")`: Giữ chỗ cho lịch sử trò chuyện, tên biến là `chat_history`.\n",
    "   - Tin nhắn người dùng (\"human\"): \"Tôi muốn đặt một chiếc pizza.\"\n",
    "\n",
    "2. **`chat_history`**:\n",
    "   - Đây là dữ liệu thực tế sẽ được \"đổ\" vào `MessagesPlaceholder`. Trong ví dụ, lịch sử trò chuyện là một danh sách các tin nhắn giữa người dùng và AI.\n",
    "\n",
    "3. **`chain.invoke`**:\n",
    "   - Khi chạy, LangChain sẽ thay thế `MessagesPlaceholder` bằng `chat_history` và gửi toàn bộ nội dung (system + chat_history + human) tới mô hình AI.\n",
    "\n",
    "##### Kết quả\n",
    "Kết quả có thể là:\n",
    "```\n",
    "Chào bạn! Bạn đã nhắc đến pizza pepperoni rồi, giờ bạn muốn kích thước bao nhiêu cho chiếc pizza này?\n",
    "```\n",
    "\n",
    "#### So sánh đơn giản\n",
    "- Không có `MessagePlaceHolder`: Bạn phải viết cứng mọi tin nhắn trong template, không linh hoạt.\n",
    "- Có `MessagePlaceHolder`: Bạn để trống một \"khe\" và thay đổi nội dung tùy theo tình huống.\n",
    "\n",
    "#### Ứng dụng thực tế\n",
    "- Chatbot: Giữ lịch sử trò chuyện giữa người dùng và AI.\n",
    "- Hỏi đáp: Chèn các câu hỏi trước đó để AI hiểu ngữ cảnh.\n",
    "- Tùy chỉnh: Thêm dữ liệu động từ cơ sở dữ liệu hoặc input của người dùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000246FEFA9090>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='Summarize the conversation so far in {word_count} words.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"human\", \"Summarize the conversation so far in {word_count} words.\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a summarization specialist AI assistant. Your mission is to summarize conversations using key points.\n",
      "Human: Hello! I’m Teddy. Nice to meet you.\n",
      "AI: Nice to meet you! I look forward to working with you.\n",
      "Human: Summarize the conversation so far in 5 words.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"Hello! I’m Teddy. Nice to meet you.\"),\n",
    "        (\"ai\", \"Nice to meet you! I look forward to working with you.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
