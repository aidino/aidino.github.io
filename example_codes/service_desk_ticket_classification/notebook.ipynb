{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5870ae0-6165-459e-9c40-0f282883be7b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 165,
    "lastExecutedAt": 1707667023665,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."
   },
   "source": [
    "CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n",
    "\n",
    "Your role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**CleverSupport là một công ty tiên phong trong lĩnh vực đổi mới AI, chuyên phát triển các giải pháp dựa trên AI để nâng cao dịch vụ hỗ trợ khách hàng. Đề án mới nhất của họ là xây dựng một hệ thống phân loại văn bản có thể tự động phân loại khiếu nại của khách hàng.**\n",
    "\n",
    "**Vai trò của bạn với tư cách là một nhà khoa học dữ liệu bao gồm việc tạo ra một mô hình học máy tinh vi có thể chính xác gán các khiếu nại vào các danh mục cụ thể, chẳng hạn như thế chấp, thẻ tín dụng, chuyển tiền, thu hồi nợ, v.v.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 5319,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1712739671517,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install torchmetrics",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa90b61-0244-4236-aa93-e33a7a088eec",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 3243,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1712740302035,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from collections import Counter\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a51a81-1301-4a80-b8c6-716faaff4c5c",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 66,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1712740302102,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "nltk.download('punkt')",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ngohongthai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b12eaf-e55c-422c-94a2-b0197c465a1b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 168,
    "lastExecutedAt": 1712740315283,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import json\nlabels = np.load('labels.npy')\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)"
   },
   "outputs": [],
   "source": [
    "# Import data and labels\n",
    "with open(\"words.json\", 'r') as f1:\n",
    "    words = json.load(f1)\n",
    "with open(\"text.json\", 'r') as f2:\n",
    "    text = json.load(f2)\n",
    "labels = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02934bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int8(0), np.int8(1), np.int8(2), np.int8(3), np.int8(4)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d630badb-23dd-4368-9a96-e2b478ad5cff",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 183,
    "lastExecutedAt": 1712740317656,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\nfor i, sentence in enumerate(text):\n    # Looking up the mapping dictionary and assigning the index to the respective words\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]"
   },
   "outputs": [],
   "source": [
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "\n",
    "# Looking up the mapping dictionary and assigning the index to the respective words\n",
    "for i, sentence in enumerate(text):\n",
    "    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "    \n",
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "text = pad_input(text, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2654836-631f-415e-9922-5ab3bafaaafa",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1712740330447,
    "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())"
   },
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "train_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\n",
    "test_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18bcaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=400\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819ab94",
   "metadata": {},
   "source": [
    "### 1. Defining the classifier model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a03f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier class\n",
    "class TicketClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, target_size):\n",
    "        super(TicketClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, target_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f64d050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "target_size = len(np.unique(labels))\n",
    "embedding_dim = 64\n",
    "\n",
    "# Create an instance of the TicketClassifier class\n",
    "model = TicketClassifier(vocab_size, embedding_dim, target_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466c783",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "605fcacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0038048693239688875\n",
      "Epoch: 2, Loss: 0.0016321743056178094\n",
      "Epoch: 3, Loss: 0.0007453516572713852\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    running_loss, num_processed = 0,0\n",
    "    for inputs, labels in train_loader:\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_processed += len(inputs)\n",
    "    print(f\"Epoch: {i+1}, Loss: {running_loss/num_processed}\")\n",
    "\n",
    "\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=5)\n",
    "precision_metric = Precision(task='multiclass', num_classes=5, average=None)\n",
    "recall_metric = Recall(task='multiclass', num_classes=5, average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95756969",
   "metadata": {},
   "source": [
    "### 3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b37e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7900000214576721\n",
      "Precision (per class): [0.6934673190116882, 0.6995305418968201, 0.8737373948097229, 0.7908163070678711, 0.9020618796348572]\n",
      "Recall (per class): [0.71875, 0.7842105031013489, 0.8009259104728699, 0.8072916865348816, 0.8333333134651184]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "predicted = []\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    output = model(inputs)\n",
    "    cat = torch.argmax(output, dim=-1)\n",
    "    predicted.extend(cat.tolist())\n",
    "    accuracy_metric(cat, labels)\n",
    "    precision_metric(cat, labels)\n",
    "    recall_metric(cat, labels)\n",
    "\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "precision = precision_metric.compute().tolist()\n",
    "recall = recall_metric.compute().tolist()\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision (per class):', precision)\n",
    "print('Recall (per class):', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68cc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "playground_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
